{
  "hash": "69fd7432a94f830629a44ad55bea9d2c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Handling Uncertainty\"\n---\n\n## Learning goals\n\n1. Understand the Bayes Theorem as a foundational block for making sense of uncertainty.\n\n## Bayesian Interpretation of Uncertainty\n\nInstead of viewing probabilities in terms of the frequencies of random, repeatable events, what is referred as the _frequentist_ interpretation of probability, one can see them as proxy for uncertainty quantification. That means, that new events serve as new data points -- evidence! -- for improving the understanding of uncertainty in a process of interest.\n\n### Bayes Equation\n\nLet us first formally define the Bayes equation.\n\n::: {#def-bayes}\n\nThe probability of an event $x$ given knowledge of an event $y$ can be expressed in the following form:\n$$\np(y\\vert x)=\\frac{p(x\\vert y)p(y)}{p(x)}\n$$ {#eq-bayes}\n\n:::\n\nThe events $x$ and $y$ can be understood as outcomes of the _random variables_ $X$ and $Y$, respectively. Random variables are functions that maps our event space $\\Omega$ to event outcomes, e.g., $x$ and $y$. When we write $p(X=x)$ we are referring to the probability that the random variable function $X$ is evaluated to $x$. When the context is clear, we can abuse the notation and just write $p(x)$ or $p(y)$.\n\n@def-bayes is a consequence of two important probability rules: _sum rule_ and _product rule_. \n\n### Probability rules: Sum Rule and Product Rule\n\nThe _sum rule_ is also known as _Law of Total Probability_. @bishopPatternRecognitionMachine2006 expresses them as:\n\n$$\np(X) = \\sum_Y p(X,Y)\n$$ {#eq-sum}\n$$\np(X,Y)=p(Y\\vert X)p(X)\n$$ {#eq-prob}\n\nWhere $p(X,Y)$ is the _joint probability_ of random variables $X$ and $Y$, also commonly expressed as their intersection $p(X\\cap Y)$.\n\nThe _product rule_ (@eq-prob) is also the definition of conditional probability.\n\n### Developing the Intuition For Understanding Uncertainty\n\nLooking at @eq-bayes, we see that the probability of $y$ happening given that $x$ happened depends on the probability of what we knew before about the process, $p(x)$, which is commonly referred as the _priors_ and the _likelihood_ of it happening, $p(x\\vert y)$. What we compute is the _posterior_, that is, the updated probability of $y$ happening given $x$ having happened.\n\n::: {#exm-anomaly-detection}\n## Anomaly Detection\nA machine is equipped with an electric current sensor and a control system that flags if the electric current is outside of the typical range. We can assign a random variable $F$ that maps boolean outcomes to events in $\\Omega$, i.e., $F: \\Omega\\rightarrow\\{f,nf\\}$, where $f$ represents at least one flagged event and $nf$ a non-flagged one, and $\\Omega$ represents the event space for workpieces being processed in the machine.\n\nLet us assume that we monitor another property of this machine, namely, how often we have to discard a workpiece being produced there. We attribute another random variable $D$ to this process that also maps $D:\\Omega\\rightarrow\\{d,nd\\}$ \n\nWe want to continuously verify how effective our anomaly detection metric is in predicting defective workpieces, that is, monitor our $p(D=d\\vert F=f)$ or simply $p(d|f)$ for convenience. This will be our _posterior_. Ideally, if we want to implement a metric as proxy for defective workpieces, this posterior should be close to $1$, i.e., all flagged events led to a defective workpiece. \n::: \n\nTo make the discussion concrete, consider the following toy scenario. Suppose that, historically, $2\\%$ of the parts produced by the machine turn out defective (a really bad rate, but useful for the demonstration). This means:\n$$\np(d)=0.02\n$$\n\nWe also happen to know from historical data that when a defect is present, the current sensor raises a flag $90\\%$ of the time, but it also generates false alarms: even when the part is fine, it still flags $15\\%$ of the time. Formally: \n$$\np(f\\vert d)  = 0.9 \\text{, }\np(f\\vert nd) = 0.15 \n$$\n\nFrom @eq-prob and @eq-sum, we can compute $p(f)$:\n\n$$\np(f)=p(f\\vert d)p(d) + p(f\\vert nd)p(nd)\n$$\n\nBayes' rule lets us update the belief about a part being defective as soon as the sensor fires. Applying @eq-bayes and since $Y$ can only assume one of the values in the set $\\{d,nd\\}$, i.e., $p(nd)=1-p(d)$, we have the following equation in function of the variables we already know:\n\n$$\np(d\\vert f)=\\frac{p(f\\vert d)p(d)}{p(f)}=\\frac{p(f\\vert d)p(d)}{p(f\\vert d)p(d) + p(f\\vert nd)(1-p(d))}\n$$\n\nThis computes to $p(d\\vert f) =$ 0\\.11, which is still surprisingly low given our prior and likelihood. The method, however, forces us to be realistic in our belief and say that we either need more _evidence_ to prove $A$ as a metric for $Y$ or we should pivot to other experiments. Since our result depended so strongly on our choice of priors, we can assume that reliably collecting this information is critically important for significant outcomes using the method. Indeed, this is a common critique about the Bayesian approach to probability: it's often more difficult to be reproduced in comparison to frequentist approaches, since the research outcome depends heavily on these initial assumptions.\n\n### Extending the example with online update\n\nThe previous example computed an analytical estimate for our posterior, making no explicit assumption about the underlying probability distribution of the prior and likelihood, treating them as fixed point estimates based on historical data.\n\nThe bayesian approach conceptualizes probabilities as uncertainty. This is useful for online learning scenarios, where part of the dataset is encountered during runtime. In order to apply online learning to @exm-anomaly-detection, let us first formally define our new learning task:\n\n$$\np(\\theta_{f,d} \\vert \\ \\mathcal{D}_f)=?\n$$\n\nWhere $\\theta_{f,d}$ is the probability of the part being defective given the sensor reading, $\\mathcal{D}_f=\\{D_1, D_2, \\dots, D_i\\}$ is the set of observations of $D$ outcomes when $F=f$. Interestingly, $\\theta_{f,d}$ is both a probability and a random variable itself, since we are uncertain about its value.\n\nWe can once again express our task in terms of the bayesian equation (@eq-bayes):\n$$\n\\begin{align*}\np(\\theta_{f,d} \\vert \\ \\mathcal{D}_f)&\\propto p(\\mathcal{D}_f\\vert \\theta_{f,d})p(\\theta_{f,d}) \\\\ \n&\\propto p(\\mathcal{D}_f\\vert \\theta_{f,d})(\\theta_{f,d})\n\\end{align*}\n$$ {#eq-exm-update}\n\n#### Modelling the prior and likelihood\n\nNow, we need ways to express both our likelihood and prior as probability distributions, instead of point estimates, since we want to update our belief as new data points are observed. For that, we use the concept of _conjugate priors_, which are pairs of probability distributions that, when used together in Bayes' rule, yield a posterior distribution of the same family as the prior distribution. This property greatly simplifies the process of updating beliefs with new evidence. \n\nIn our case, we can use the Beta distribution as our prior and the Bernoulli distribution as our likelihood. Specifically, we can model the prior distribution of $\\theta_{f,d}$ as a Beta distribution, $Beta(a, b)$, and the likelihood of the observed data as a Bernoulli distribution, $Ber(\\theta_{f,d})$.\n\n$$\n\\mathcal{D}_f\\vert \\theta_{f,d} \\sim Ber(\\theta_{f,d})\n$$\n$$\n\\theta_{f,d} \\sim Beta(a, b)\n$$\n\nWhere $\\sim$ means that the random variable follows the probability distribution on the right side. The Beta distribution is defined on the interval $[0, 1]$ and is parameterized by two positive shape parameters, $a$ and $b$, commonly referred as hyperparameters and they are used to encode our beliefs into the prior distribution. It is often used to model the distribution of probabilities. The Bernoulli distribution, on the other hand, models binary outcomes (success/failure) and is parameterized by a single probability parameter $\\theta$.\n\nWe can write our likelihood and prior as:\n$$p(\\mathcal{D}_f\\vert \\theta_{f,d})=\\theta_{f,d}^{N_{f,d}}(1-\\theta_{f,d})^{N_{f,nd}}$$\n$$p(\\theta_{f,d})\\propto \\theta_{f,d}^{a-1}(1-\\theta_{f,d})^{b-1}$$\n\nHere, $N_{f,d}$ and $N_{f,nd}$ are the counts of flagged defective and non-defective events. Plugging in these expressions into @eq-exm-update, we get:\n$$\n\\begin{align*}\np(\\theta_{f,d} \\vert \\ \\mathcal{D}_f)&\\propto Ber(\\theta_{f,d})Beta(a, b) \\\\\n&\\propto \\theta_{f,d}^{N_{f,d}}(1-\\theta_{f,d})^{N_{f,nd}}\\theta_{f,d}^{a-1}(1-\\theta_{f,d})^{b-1} \\\\\n&\\propto \\theta_{f,d}^{N_{f,d}+a-1}(1-\\theta_{f,d})^{N_{f,nd}+b-1} \\\\\n&\\propto Beta(a+N_{f,d}, b+N_{f,nd})\n\\end{align*}\n$$ {#eq-exm-update-2}\n\nWe see that the posterior also follows a Beta distribution, with updated parameters $a+N_{f,d}$ and $b+N_{f,nd}$. This means that we can update our belief about the probability of a part being defective by simply updating the parameters of the Beta distribution as we observe new data. Therefore, Bayesian inference can be used both sequentially or in batch mode with equivalent results, which it makes it suitable for online learning scenarios. For a proof of this property, see @murphyMachineLearningProbabilistic2014. From Murphy's book, we can also look at useful properties of the Beta distribution, such as its mean, mode and variance:\n$$\n\\mathbb{E}[\\theta]=\\frac{a}{a+b}, \\quad mode(\\theta)=\\frac{a-1}{a+b-2}, \\quad Var(\\theta)=\\frac{ab}{(a+b)^2(a+b+1)}\n$$ {#eq-beta-props}\n\nBased on that, we can compute the mode, or MAP estimate, of our posterior distribution as:\n$$\n\\begin{align*}\n\\hat{\\theta}_{f,d}\\vert \\ \\mathcal{D}_f&=\\frac{a+N_{f,d}-1}{a+b+N_{f,d}+N_{f,nd}-2} \\\\\n&=\\frac{a+N_{f,d}-1}{a+b+N_f-2} \\\\\n\\end{align*}\n$$ {#eq-exm-map}\n\nWhere $N_f=N_{f,d}+N_{f,nd}$ is the total number of flagged events. Let us plot the prior, likelihood and posterior distributions for our initial example. We can choose the hyperparameters $a$ and $b$ such that the prior mean matches our initial belief of $p(d)=0.02$. A good choice is to set $a=2$ and $b=98$, which gives us the same prior mean of $2/(2+98)=0.02$. For the likelihood, we can use the counts of flagged defective and non-defective events based on our initial example. Assuming we observed $90$ flagged defective events and $15$ flagged non-defective events, we have $N_{f,d}=90$ and $N_{f,nd}=15$. Thus, our posterior will have parameters $a+N_{f,d}=2+90=92$ and $b+N_{f,nd}=98+15=113$.\n\n::: {#89f9112a .cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![](handling-uncertainty_files/figure-html/cell-2-output-1.png){width=656 height=375}\n:::\n:::\n\n\nTo see how new observations gradually nudge our posterior beliefs, the next simulation follows 1000 consecutive parts using the @algo-beta-bernoulli-update. After each part, the posterior is updated and the mode and the error bar is computed based on @eq-beta-props. The error bar is computed as three times the standard deviation ($3\\sigma$) around the mode, assuming a latent true latent defect probability of flagged events of $\\theta_{f,d}^\\star=0.6$.\n\n::: {#e0e00040 .cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![](handling-uncertainty_files/figure-html/cell-3-output-1.png){width=758 height=374}\n:::\n:::\n\n\nOur error bar shrinks as we observe more data, reflecting our increased confidence in the estimate. The mode converges towards the true latent defect probability of flagged events, $\\theta_{f,d}^\\star=0.6$, demonstrating how Bayesian updating refines our beliefs with accumulating evidence.\n\n```pseudocode\n#| label: algo-beta-bernoulli-update\n\n\\begin{algorithm}\n\\caption{Sequential Beta-Bernoulli Update for $\\theta_{f,d}$}\n\\begin{algorithmic}[1]\n\\INPUT $a_0 \\geq 1, b_0 \\geq 1$, prior Beta hyperparameters\n\\INPUT $\\{d_i\\}_{i=1}^T$, sequence of flagged defect observations\n\\OUTPUT $\\{\\hat{\\theta}_{f,d}(t)\\}_{t=1}^T$, $\\{\\sigma(t)\\}_{t=1}^T$\n\\Procedure{BetaBernoulliUpdate}{$a_0, b_0, \\{d_i\\}_{i=1}^T$}\n  \\State $a \\leftarrow a_0$ \\Comment{initialize posterior parameters}\n  \\State $b \\leftarrow b_0$\n  \\For{$t = 1$ \\To $T$}\n    \\State $d \\leftarrow d_i$ \\Comment{observe whether part is defective}\n    \\If{$d = \\texttt{defect}$}\n      \\State $a \\leftarrow a + 1$ \\Comment{increment success count}\n    \\Else\n      \\State $b \\leftarrow b + 1$ \\Comment{increment failure count}\n    \\EndIf\n    \\State $\\hat{\\theta}_{f,d}(t) \\leftarrow \\dfrac{a - 1}{a + b - 2}$ \\Comment{MAP estimate (mode)}\n    \\State $\\sigma^2(t) \\leftarrow \\dfrac{ab}{(a+b)^2(a+b+1)}$ \\Comment{posterior variance}\n    \\State $\\sigma(t) \\leftarrow \\sqrt{\\sigma^2(t)}$\n  \\EndFor\n  \\State \\textbf{return} $\\{\\hat{\\theta}_{f,d}(t)\\}_{t=1}^T, \\{\\sigma(t)\\}_{t=1}^T$\n\\EndProcedure\n\\end{algorithmic}\n\\end{algorithm}\n```\n\n::: {.callout-note}\n## Generalization\nThis algorithm assumes $a_0 \\geq 1$ and $b_0 \\geq 1$, which corresponds to using the Beta distribution to encode pseudo-counts or prior observations. For more general priors (e.g., Jeffreys prior with $a_0 = b_0 = 0.5$), the MAP estimate should be replaced with the posterior mean $\\frac{a}{a+b}$ when $a \\leq 1$ or $b \\leq 1$, since the mode is undefined in those cases.\n:::\n\n",
    "supporting": [
      "handling-uncertainty_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}