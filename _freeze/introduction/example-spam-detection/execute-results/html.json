{
  "hash": "91632b43636ecd4898dbf911175f9100",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Example: Spam Detection\"\n---\n\n::: {#exm-spam-detection}\nA spam detection system classifies incoming messages as spam or non-spam based on the presence of specific words. The system uses a fixed vocabulary and is trained on example messages labeled as spam or non-spam.\n\n::: {layout-ncol=2}\n\n::: {#tbl-spam-vocab .cell tbl-cap='Vocabulary' execution_count=1}\n\n::: {.cell-output .cell-output-display .cell-output-markdown execution_count=1}\n  id  word\n----  --------\n   0  word\n   1  secret\n   2  offer\n   3  low\n   4  price\n   5  valued\n   6  customer\n   7  today\n   8  dollar\n   9  million\n  10  sports\n  11  is\n  12  for\n  13  play\n  14  healthy\n  15  pizza\n:::\n:::\n\n\n::: {#tbl-spam-data .cell tbl-cap='Spam dataset' execution_count=2}\n\n::: {.cell-output .cell-output-display .cell-output-markdown execution_count=2}\nlabel     message\n--------  ------------------------------------\nspam      million dollar offer\nspam      secret offer today\nspam      secret is secret\nspam      low price for secret valued customer\nnon-spam  play sports today\nnon-spam  sports costs dollar\nnon-spam  healthy pizza for customer\nnon-spam  low price for valued customer\nnon-spam  play secret sports today\nnon-spam  sports is healthy\nnon-spam  low price pizza\n:::\n:::\n\n\n:::\n\nThis example is based on the exercise 3.22 from @murphyMachineLearningProbabilistic2014.\n:::\n\n**Goal.** Our goal is to build a spam detection system capable of classifying the messages as spam or not spam. Let us formulate the task:\n\n$$\np(y_i = c \\vert \\mathbf{x}_i, \\pmb{\\Theta}) = ? \n$$\n\nWhere $p(y_i = c)$ is the probability of the message $i$ being of class $c$ in $\\mathcal{C}$; $\\pmb{\\Theta}$ is the collection of all model parameters; $\\mathbf{x}_i$ is the vector of features we want to classify, i.e., the output of some kind of builder $B$ that decomposes the message from @tbl-spam-data in terms of $\\mathcal{V}$ from @tbl-spam-vocab. Expressing the task in terms of @eq-bayes, we get:\n\n$$\np(y_i = c \\vert \\mathbf{x}_i, \\pmb{\\Theta}) \\propto p(\\mathbf{x}_i\\vert y_i=c, \\pmb{\\Theta})p(y_i=c\\vert \\pmb{\\Theta})\n$$ {#eq-exm-spam-task}\n\n**Building the feature vector $\\mathbf{x}$.** Since we only have two classes, spam and non-spam, let us apply a builder $B_\\mathbb{I}: \\mathcal{D} \\to \\{0,1\\}^{|\\mathcal{V}|}$ that transforms the data set into binary feature vectors indicating the presence or absence of each word in the vocabulary. With this transformation, we can model each $x_i$ as a joint distribution of Bernoulli distributions of the corresponding feature for each word in $\\mathcal{V}$, $$x_{ij}\\vert y_i=c \\sim ber(x_{ij}\\vert \\theta_{jc}).$$ \n**Modelling the likelihood.** Assuming that all $x_{ij}$ are conditionally independent given the classes $C$, we can express the likelihood as:\n$$\n\\forall x_a, x_b \\in \\mathbf{x}_i, x_a \\perp x_b \\vert y_i=c \\implies\\\\\np(\\mathbf{x}_i\\vert y_i=c, \\pmb{\\Theta}) = \\prod_j ber(x_{ij}\\vert \\theta_{jc})\n$$ {#eq-exm-spam-likelihood}\n\nwhere \n\n$$\\pmb{\\theta}= \\{\\theta_{jc}\\}_{j \\in \\mathcal{V}, c \\in \\mathcal{C}} \\quad \\theta_{jc} = p(x_{ij}=1\\vert y_i=c) \\quad \\forall \\theta_{jc} \\in \\pmb{\\Theta}$$\n\nare the probabilities of words indexed by $j$ appearing in messages of class $c$.\n\n**Modelling the prior**. To complete @eq-exm-spam-task, we also need to model the prior $p(y_i=c\\vert \\pmb{\\Theta})$. Since for each message $i$ we only have one class $y_i$, we can model it using a categorical distribution:\n$$y_i \\mid \\pmb{\\pi} \\sim cat(y_i\\vert \\pmb{\\pi})$$\n\nwhere \n$$\\pmb{\\pi} = \\{\\pi_c\\}_{c \\in \\mathcal{C}} \\quad \\pi_c = p(y_i=c) \\quad \\forall \\pi_c \\in \\pmb{\\Theta}$$\n\nare the prior class probabilities for each class in $\\mathcal{C}$.\n\n## Training the model: Computing $\\pmb{\\Theta}$ via MLE\n\nTo use the equations above, we first need to compute our parameters $\\pmb{\\Theta}$. For that, we want to use our dataset from @tbl-spam-data represented as $\\mathcal{D}$ to learn the parameters $\\pmb{\\Theta}$ of our model. Applying a similar bayesian treatment, we get:\n$$\np(\\pmb{\\Theta}\\vert \\mathcal{D}) \\propto p(\\mathcal{D}\\vert \\pmb{\\Theta})p(\\pmb{\\Theta})\n$$ {#eq-exm-spam-training-likelihood}\n\nWe can use the maximum likelihood estimate (MLE) to compute the parameters in $\\pmb{\\Theta}$. Later on, we will discuss the limitations of this approach and how we can improve it.\n\n**Computing the training likelihood $p(\\mathcal{D}\\vert\\pmb{\\Theta})$.** Since $\\mathcal{D}\\subseteq \\mathbf{X}\\times \\mathcal{Y}$, we can express the equation using @Thm-product-rule:\n\n$$\n\\begin{align}\np(\\mathcal{D}\\mid \\pmb{\\Theta}) &= \\prod_{i} p(\\mathbf{x}_i,y_i\\mid \\pmb{\\Theta}) \\\\\n&= \\prod_{i} p(y_i\\mid \\pmb{\\pi}) p(\\mathbf{x}_i\\mid y_i, \\pmb{\\theta})\n\\end{align}\n$$ {#eq-exm-spam-training-likelihood-aggregated}\n\nsince we assume i.i.d. data points. The idea is to to use the log likelihood to compute the MLE for each term in $\\Theta$. We can further simplify the expression by using @prp-categorical-indicators and @cor-categorical-one-hot-indicator-conditional as\n\n$$\np(\\mathcal{D}\\mid \\pmb{\\Theta}) = \\prod_{i} \\prod_{c} [\\pi_c^{\\mathbb{I}(y_i=c)} \\prod_j \\text{ber}(x_{ij}\\mid \\pmb{\\theta})^{\\mathbb{I}(y_i=c)}]\n$$ {#eq-exm-spam-training-likelihood-aggregated-2}\n\nTaking the logarithm from @eq-exm-spam-training-likelihood-aggregated-2, and taking the logarithm to simplify the product into a sum, we get:\n$$\n\\log p(\\mathcal{D}\\vert \\pmb{\\Theta}) = \\sum_{i} \\sum_c \\mathbb{I}(y_i=c) \\log \\pi_c + \\sum_{i} \\sum_j \\sum_c [ \\mathbb{I}(y_i=c) \\log \\theta_{jc} +(1-\\mathbb{I}(y_i=c)) \\log (1-\\theta_{jc})]\n$$ {#eq-exm-spam-training-log-likelihood}\n\n**Computing the MLE for $\\theta_{jc}$.** To compute the MLE for $\\theta_{jc}$, we need to maximize the log-likelihood from @eq-exm-spam-training-log-likelihood with respect to the parameters $\\theta_{jc}$.\n\nTo do this, we can take the derivative of the log-likelihood with respect to $\\theta_{jc}$, set it to zero, and solve for $\\theta_{jc}$. This gives us the following equation:\n\n$$\n\\frac{\\partial}{\\partial \\theta_{jc}} \\log p(\\mathcal{D}\\vert \\pmb{\\Theta}) = \\sum_{i} \\sum_c \\mathbb{I}(y_i=c) \\frac{1}{\\theta_{jc}} - \\sum_{i} \\sum_c (1-\\mathbb{I}(y_i=c)) \\frac{1}{1-\\theta_{jc}} = 0\n$$\n\nWhich leads to the MLE for $\\theta_{jc}$ as follows:\n\n$$\n\\begin{aligned}\n\\hat{\\theta}_{jc} &= \\frac{\\sum_{i} x_{ij} \\mathbb{I}(y_i=c)}{\\sum_{i} \\mathbb{I}(y_i=c)}\n\\end{aligned}\n$$\n\nTo compute the matrix of probabilities $\\pmb{\\theta}$, we can use the maximum likelihood estimate (MLE) for each word $j$ in the vocabulary and each class $c$:\n\n$$\n\\hat{\\theta}_{jc} = \\frac{\\sum_{i} x_{ij} \\mathbb{I}(y_i=c)}{\\sum_{i} \\mathbb{I}(y_i=c)}\n$$ {#eq-exm-spam-mle}\n\nWhere $\\mathbb{I}(\\cdot)$ is the indicator function that returns 1 if the condition is true and 0 otherwise.\n\n**Computing the MLE for $\\pi_c$.** Similarly, to compute the MLE for the prior class probabilities $\\pi_c$, we need to maximize the log-likelihood from @eq-exm-spam-training-log-likelihood with respect to the parameters $\\pi_c$.\n\nTaking the derivative of the log-likelihood with respect to $\\pi_c$ and setting it to zero (subject to the constraint that $\\sum_c \\pi_c = 1$), we get:\n\n$$\n\\frac{\\partial}{\\partial \\pi_c} \\log p(\\mathcal{D}\\vert \\pmb{\\Theta}) = \\sum_{i} \\mathbb{I}(y_i=c) \\frac{1}{\\pi_c} = 0\n$$\n\nWhich leads to the MLE for $\\pi_c$ as follows:\n\n$$\n\\hat{\\pi}_c = \\frac{\\sum_{i} \\mathbb{I}(y_i=c)}{N}\n$$ {#eq-exm-spam-mle-prior}\n\nwhere $N$ is the total number of training examples. In other words, the MLE for the prior class probability is simply the fraction of training examples that belong to class $c$. \n\n::: {#chl-black-swan}\n\n## Black Swan\n\nHowever, using the MLE directly suffers from the **black swan problem**: This is analogous to the classical black swan problem: just because we've never observed a black swan doesn't mean the probability of seeing one should be exactly zero. It assigns extreme probabilities (exactly 0 or 1) when a word never appears or always appears in messages of a particular class. This leads to several issues:\n\n1. **Zero probabilities** ($\\theta_{jc} = 0$) cause numerical problems, e.g. when computing log-probabilities, since $\\log(0) = -\\infty$.\n2. **Probabilities of 1** ($\\theta_{jc} = 1$) make the model overly confident and brittle. If a word that always appeared in spam suddenly appears in a non-spam message, the likelihood becomes exactly 0, since we compute the product, completely ignoring all other evidence.\n3. **Poor generalization**: The model becomes unable to handle unseen word-class combinations, even when they are plausible.\n\n:::\n\n**Laplace Smoothing.** To address @chl-black-swan, we can use **Laplace smoothing** (also called add-one smoothing):\n\n$$\n\\hat{\\theta}_{jc} = \\frac{\\sum_{i} x_{ij} \\mathbb{I}(y_i=c) + \\alpha}{\\sum_{i} \\mathbb{I}(y_i=c) + \\alpha |\\mathcal{V}|}\n$$ {#eq-exm-spam-mle-smoothed}\n\nwhere $\\alpha > 0$ is the smoothing parameter (typically $\\alpha=1$) and $|\\mathcal{V}|$ is the size of the vocabulary. This ensures all probabilities are strictly in the interval $(0, 1)$, preventing both numerical instability and overconfident predictions.\n\nLet us compute the matrix $\\pmb{\\hat{\\theta}}$ using the dataset from @tbl-spam-data, considering that we have two classes: \"spam\" and \"non-spam\", where $c=1$ for \"spam\" and $c=0$ for \"non-spam\".\n\n::: {#tbl-spam-theta .cell tbl-cap='Matrix of probabilities $\\pmb{\\hat{\\theta}}$' execution_count=3}\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"2\" halign=\"left\">W/ Laplace Smoothing</th>\n      <th colspan=\"2\" halign=\"left\">W/O Laplace Smoothing</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>non-spam</th>\n      <th>spam</th>\n      <th>non-spam</th>\n      <th>spam</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>word</th>\n      <td>0.043478</td>\n      <td>0.05</td>\n      <td>0.000000</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>secret</th>\n      <td>0.086957</td>\n      <td>0.25</td>\n      <td>0.142857</td>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <th>offer</th>\n      <td>0.043478</td>\n      <td>0.15</td>\n      <td>0.000000</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>low</th>\n      <td>0.130435</td>\n      <td>0.10</td>\n      <td>0.285714</td>\n      <td>0.25</td>\n    </tr>\n    <tr>\n      <th>price</th>\n      <td>0.130435</td>\n      <td>0.10</td>\n      <td>0.285714</td>\n      <td>0.25</td>\n    </tr>\n    <tr>\n      <th>valued</th>\n      <td>0.086957</td>\n      <td>0.10</td>\n      <td>0.142857</td>\n      <td>0.25</td>\n    </tr>\n    <tr>\n      <th>customer</th>\n      <td>0.130435</td>\n      <td>0.10</td>\n      <td>0.285714</td>\n      <td>0.25</td>\n    </tr>\n    <tr>\n      <th>today</th>\n      <td>0.130435</td>\n      <td>0.10</td>\n      <td>0.285714</td>\n      <td>0.25</td>\n    </tr>\n    <tr>\n      <th>dollar</th>\n      <td>0.086957</td>\n      <td>0.10</td>\n      <td>0.142857</td>\n      <td>0.25</td>\n    </tr>\n    <tr>\n      <th>million</th>\n      <td>0.043478</td>\n      <td>0.10</td>\n      <td>0.000000</td>\n      <td>0.25</td>\n    </tr>\n    <tr>\n      <th>sports</th>\n      <td>0.217391</td>\n      <td>0.05</td>\n      <td>0.571429</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>is</th>\n      <td>0.086957</td>\n      <td>0.10</td>\n      <td>0.142857</td>\n      <td>0.25</td>\n    </tr>\n    <tr>\n      <th>for</th>\n      <td>0.130435</td>\n      <td>0.10</td>\n      <td>0.285714</td>\n      <td>0.25</td>\n    </tr>\n    <tr>\n      <th>play</th>\n      <td>0.130435</td>\n      <td>0.05</td>\n      <td>0.285714</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>healthy</th>\n      <td>0.130435</td>\n      <td>0.05</td>\n      <td>0.285714</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>pizza</th>\n      <td>0.130435</td>\n      <td>0.05</td>\n      <td>0.285714</td>\n      <td>0.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThe resulting matrix $\\pmb{\\theta}$ indicates the probability of each word in the vocabulary given the class (spam or non-spam). For example, the word \"secret\" has a high probability of appearing in spam messages, while \"sports\" is more likely to appear in non-spam messages.\n\n## Making Predictions\n\nTo make predictions on new messages, we can use the learned parameters from @tbl-spam-theta and apply @eq-exm-spam-task. For the existing messages in @tbl-spam-data, we can compute the posterior probabilities for each class and classify the messages accordingly.\n\n::: {#tbl-spam-predictions .cell tbl-cap='Predictions on the spam dataset' execution_count=4}\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>message</th>\n      <th>true_label</th>\n      <th>predicted_label</th>\n      <th>posteriors</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>million dollar offer</td>\n      <td>spam</td>\n      <td>spam</td>\n      <td>{'non-spam': '1.53e-05', 'spam': '2.08e-04'}</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>secret offer today</td>\n      <td>spam</td>\n      <td>spam</td>\n      <td>{'non-spam': '5.05e-05', 'spam': '6.25e-04'}</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>secret is secret</td>\n      <td>spam</td>\n      <td>non-spam</td>\n      <td>{'non-spam': '6.72e-05', 'spam': '1.18e-03'}</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>low price for secret valued customer</td>\n      <td>spam</td>\n      <td>spam</td>\n      <td>{'non-spam': '3.57e-07', 'spam': '5.39e-07'}</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>play sports today</td>\n      <td>non-spam</td>\n      <td>non-spam</td>\n      <td>{'non-spam': '4.86e-04', 'spam': '2.94e-05'}</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>sports costs dollar</td>\n      <td>non-spam</td>\n      <td>spam</td>\n      <td>{'non-spam': '2.06e-03', 'spam': '5.59e-04'}</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>healthy pizza for customer</td>\n      <td>non-spam</td>\n      <td>non-spam</td>\n      <td>{'non-spam': '3.94e-05', 'spam': '3.27e-06'}</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>low price for valued customer</td>\n      <td>non-spam</td>\n      <td>non-spam</td>\n      <td>{'non-spam': '3.75e-06', 'spam': '1.62e-06'}</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>play secret sports today</td>\n      <td>non-spam</td>\n      <td>spam</td>\n      <td>{'non-spam': '4.63e-05', 'spam': '9.80e-06'}</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>sports is healthy</td>\n      <td>non-spam</td>\n      <td>non-spam</td>\n      <td>{'non-spam': '3.09e-04', 'spam': '2.94e-05'}</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>low price pizza</td>\n      <td>non-spam</td>\n      <td>spam</td>\n      <td>{'non-spam': '2.63e-04', 'spam': '6.21e-05'}</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nFrom @tbl-spam-predictions, we can identify a few issues with our current model. The most glaring one is that our posteriors are extremely low, which could be a sign of numerical underflow when multiplying many small probabilities together. This is a common issue in Naive Bayes classifiers, increasing with the size of vocabulary, since all probabilities of the likelihood $\\sum_x p(\\mathbf{x}\\vert y)$ should add to $1$. To mitigate this, we could consider using log for out computations, as shown in the next section.\n\n:::{#chl-numerical-underflow}\n\n## Numerical Underflow\n\nNumerical underflow occurs when small numbers are rounded to zero in a computer. In the context of our spam detection model, when we compute the likelihood $p(\\mathbf{x}_i\\vert y_i=c, \\pmb{\\theta})$ as a product of many small probabilities (especially with a large vocabulary), the result can become so small that it is effectively treated as zero by the computer. This leads to incorrect posterior probabilities and unreliable classifications.\n:::\n\n\n## Avoiding Numerical Underflow\n\n**Logarithms as a Solution for Numerical Underflow.** To avoid numerical underflow, we can compute the logarithm of the posterior probabilities instead of the probabilities themselves. This transforms the product of probabilities into a sum of logarithms, which is numerically more stable. The logarithm of the posterior probability can be expressed as:\n\n$$\n\\begin{align*}\n\\log p(y_i = c \\vert \\mathbf{x}_i, \\pmb{\\theta}) &\\propto \\log p(\\mathbf{x}_i\\vert y_i=c, \\pmb{\\theta}) + \\log p(y_i=c\\vert \\pmb{\\theta}) \\\\\n&\\propto \\sum_j \\left[ x_{ij} \\log \\theta_{jc} + (1-x_{ij}) \\log (1-\\theta_{jc}) \\right] + \\log \\hat{\\pi}_c\n\\end{align*}\n$$ {#eq-exm-spam-log-posterior}\n\nThat means, we can maximize the log-posterior instead of the posterior itself and use it to classify messages. Let us recompute the predictions using log-probabilities.\n\n::: {#tbl-spam-log-predictions .cell tbl-cap='Predictions using log-probabilities' execution_count=5}\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>message</th>\n      <th>true_label</th>\n      <th>predicted_label</th>\n      <th>log_posteriors</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>million dollar offer</td>\n      <td>spam</td>\n      <td>spam</td>\n      <td>{'non-spam': '-1.11e+01', 'spam': '-8.48e+00'}</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>secret offer today</td>\n      <td>spam</td>\n      <td>non-spam</td>\n      <td>{'non-spam': '-9.89e+00', 'spam': '-7.38e+00'}</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>secret is secret</td>\n      <td>spam</td>\n      <td>non-spam</td>\n      <td>{'non-spam': '-9.61e+00', 'spam': '-6.74e+00'}</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>low price for secret valued customer</td>\n      <td>spam</td>\n      <td>non-spam</td>\n      <td>{'non-spam': '-1.48e+01', 'spam': '-1.44e+01'}</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>play sports today</td>\n      <td>non-spam</td>\n      <td>non-spam</td>\n      <td>{'non-spam': '-7.63e+00', 'spam': '-1.04e+01'}</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>sports costs dollar</td>\n      <td>non-spam</td>\n      <td>spam</td>\n      <td>{'non-spam': '-6.19e+00', 'spam': '-7.49e+00'}</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>healthy pizza for customer</td>\n      <td>non-spam</td>\n      <td>spam</td>\n      <td>{'non-spam': '-1.01e+01', 'spam': '-1.26e+01'}</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>low price for valued customer</td>\n      <td>non-spam</td>\n      <td>spam</td>\n      <td>{'non-spam': '-1.25e+01', 'spam': '-1.33e+01'}</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>play secret sports today</td>\n      <td>non-spam</td>\n      <td>non-spam</td>\n      <td>{'non-spam': '-9.98e+00', 'spam': '-1.15e+01'}</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>sports is healthy</td>\n      <td>non-spam</td>\n      <td>non-spam</td>\n      <td>{'non-spam': '-8.08e+00', 'spam': '-1.04e+01'}</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>low price pizza</td>\n      <td>non-spam</td>\n      <td>spam</td>\n      <td>{'non-spam': '-8.24e+00', 'spam': '-9.69e+00'}</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n**Not as expected?** We see that the number of false predictions for the second model is 4, while for the log-probability model is 7. Our log model performs worse! This is likely due to the extremely low posterior probabilities in both models, which makes it difficult to distinguish between classes. To improve performance, we may need to revisit our assumptions.\n\n## Relaxing assumption for the prior\n\nIn @eq-exm-spam-mle-prior, we assumed a uniform prior for the classes. However, this may not reflect the true distribution of spam and non-spam messages in practice. Let us investigate how we can relax this assumption.\n\n::: {.callout-note}\n\n#TODO #12 Section: Relaxing Assumption for the prior, Example Spam Detection\n\n:::\n\n",
    "supporting": [
      "example-spam-detection_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js\" integrity=\"sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js\" integrity=\"sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}