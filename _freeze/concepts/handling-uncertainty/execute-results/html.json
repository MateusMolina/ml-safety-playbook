{
  "hash": "4aeb699bce0b6cb32d5e07c986456ad9",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Handling Uncertainty\"\n---\n\n## Learning goals\n\n1. Understand the Bayes Theorem as a foundational block for making sense of uncertainty.\n\n## Bayesian Interpreation of Uncertainty\n\nInstead of viewing probabilities in terms of the frequencies of random, repeatabale events, what is referred as the _frequentist_ interpretation of probability, one can see them as proxy for uncertainty quantification. That means, that new events serve as new data points -- evidence! -- for improving the understanding of uncertainty in a process of interest.\n\n### Bayes Theorem\n\nLet us first formally define the Bayes Theorem\n\n::: {#def-bayes}\n\nThe probability of an event $x$ given knowledge of an event $y$ can be expressed in the following form:\n$$\np(y\\vert x)=\\frac{p(x\\vert y)p(y)}{p(x)}\n$$ {#eq-bayes}\n\n:::\n\nThe events $x$ and $y$ can be understood as outcomes of the _random variables_ $X$ and $Y$, respectively. Random variables are functions that maps our event space $\\Omega$ to event outcomes, e.g., $x$ and $y$. When we write $p(X=x)$ we are referering to the probability that the random variable function $X$ is evaluated to $x$. When the context is clear, we can abuse the notation and just write $p(x)$ or $p(y)$.\n\n@def-bayes is a consequence of two important probability rules: _sum rule_ and _product rule_. \n\n### Probability rules: Sum Rule and Product Rule\n\nThe _sum rule_ is also known as _Law of Total Probability_. @bishopPatternRecognitionMachine2006 expresses them as:\n\n$$\np(X) = \\sum_Y p(X,Y)\n$$ {#eq-sum}\n$$\np(X,Y)=p(Y\\vert X)p(X)\n$$ {#eq-prob}\n\nWhere $p(X,Y)$ is the _joint probability_ of random variables $X$ and $Y$, also commonly expressed as their intersection $p(X\\cap Y)$.\n\nThe _product rule_ (@eq-prob) is also the definition of conditional probability.\n\n### Developing the Intuition For Understanding Uncertainty\n\nLooking at @eq-bayes, we see that the probability of $y$ happening given that $x$ happened depends on the probability of what we knew before about the process, $p(x)$, which is commonly referred as the _priors_ and the _likelihood_ of it happening, $p(x\\vert y)$. What we compute is the _posterior_, that is, the updated probability of $y$ happening given $x$ having happened.\n\n::: {#exm-anomaly-detection}\n## Anomaly Detection\nA machine is equipped with a electric current sensor and a control system that flags if the electric current is outside of the typical range. We can assign a random variable $A$ (anomaly) that maps boolean outcomes to events in $\\Omega$, i.e., $A: \\Omega\\rightarrow\\{f,nf\\}$, where $F$ represents at least a flagged event and $NF$ a non-flagged one, and $\\Omega$ represents the event space for workpieces being processed in the machine.\n\nLet us assume that we monitor another property of this machine, namely, how often we have to discard a workpiece being produced there. We attribute another random variable $Y$ (outcome) to this processes that also maps $Y:\\Omega\\rightarrow\\{d,nd\\}$ \n\nWe want to continuously verify how effective is our anomaly detection metric in predicting defective workpieces, that is, monitor our $p(Y=d\\vert A=f)$ or simply $p(d,f)$ for convenience. This will be our _posterior_. Ideally, if we want to implement a metric as proxy for defective workpieces, this posterior should be close to $1$, i.e., all flagged events led to a defective workpiece. \n::: \n\nTo make the discussion concrete, consider the following toy scenario. Suppose that, historically, $2\\%$ of the parts produced by the machine turn out defective (a really bad rate, but useful for the demonstration). This means:\n$$\np(d)=0.02\n$$\n\nWe also happen to know from historical data that when a defect is present, the current sensor raises a flag $90\\%$ of the time, but it also generates false alarms: even when the part is fine, it still flags $15\\%$ of the time. Formally: \n$$\np(f\\vert d)  = 0.9 \\text{, }\np(f\\vert nd) = 0.15 \n$$\n\nFrom @eq-prob and @eq-sum, we can compute $p(f)$:\n\n$$\np(f)=p(f\\vert d)p(d) + p(f\\vert nd)p(nd)\n$$\n\nBayes' rule lets us update the belief about a part being defective as soon as the sensor fires. Applying @eq-bayes and since $Y$ can only assume one of the values in the set $\\{d,nd\\}$, i.e., $p(nd)=1-p(d)$, we have the following equation in function of the variables we already know:\n\n$$\np(d\\vert f)=\\frac{p(f\\vert d)p(d)}{p(f)}=\\frac{p(f\\vert d)p(d)}{p(f\\vert d)p(d) + p(f\\vert nd)(1-p(d))}\n$$\n\nThis computes to $p(d\\vert f) =$ 0\\.11, which is still surprisingly low given our prior and likelihood. The method, however, forces us to be realistic in our belief and say that we either need more _evidence_ to prove $A$ as a metric for $Y$ or we should pivot to other experiments. Since our result depended so strongly on our choice of priors, we can assume that reliabily collecting this information is critically important for significant outcomes using the method. Indeed, this is a common critique about the Bayesian approach to probability: it's often more difficult to be reproduced in comparison to frequentist approaches, since the research outcome depends heavily on these initial assumptions.\n\nTo see how new observations gradually nudge our belief, the next simulation follows 1000 consecutive parts using the @algo-bayesian-update. After each part, we recompute the quantities in Bayes' rule using the empirical frequencies observed so far.\n\n::: {#31188d87 .cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![](handling-uncertainty_files/figure-html/cell-2-output-1.png){width=653 height=342}\n:::\n:::\n\n\n```pseudocode\n#| label: algo-bayesian-update\n\n\\begin{algorithm}\n\\caption{Sequential Bayesian Update for $p(d\\mid f)$}\n\\begin{algorithmic}[1]\n\\Require $\\alpha_d, \\beta_d$ \\Comment{prior counts for $p(d)$}\n\\Require $\\alpha_{f\\mid d}, \\beta_{f\\mid d}$ \\Comment{prior counts for $p(f\\mid d)$}\n\\Require $\\alpha_{f\\mid nd}, \\beta_{f\\mid nd}$ \\Comment{prior counts for $p(f\\mid nd)$}\n\\Require $\\{(f_i, d_i)\\}_{i=1}^n$ \\Comment{sequence of observations}\n\\Ensure $\\{\\hat{p}_{d\\mid f}(i)\\}_{i=1}^n$ \\Comment{posterior after each observation}\n\\Procedure{UpdatePosterior}{$\\alpha_d, \\beta_d, \\alpha_{f\\mid d}, \\beta_{f\\mid d}, \\alpha_{f\\mid nd}, \\beta_{f\\mid nd}, \\{(f_i, d_i)\\}_{i=1}^n$}\n  \\State $c_d \\leftarrow 0$ \\Comment{defects observed}\n  \\State $c_{f\\wedge d} \\leftarrow 0$ \\Comment{flagged defects}\n  \\State $c_{f\\wedge nd} \\leftarrow 0$ \\Comment{false alarms}\n  \\For{$i = 1$ \\To $n$}\n    \\State $(f, d) \\leftarrow (f_i, d_i)$ \\Comment{$f_i \\in \\{\\texttt{flag}, \\texttt{no\\_flag}\\}$}\n    \\State $c_d \\leftarrow c_d + \\mathbb{I}[d = \\texttt{defect}]$\n    \\State $c_{nd} \\leftarrow i - c_d$\n    \\If{$f = \\texttt{flag}$}\n      \\If{$d = \\texttt{defect}$}\n        \\State $c_{f\\wedge d} \\leftarrow c_{f\\wedge d} + 1$\n      \\Else\n        \\State $c_{f\\wedge nd} \\leftarrow c_{f\\wedge nd} + 1$\n      \\EndIf\n    \\EndIf\n    \\State $\\hat{p}_d \\leftarrow \\dfrac{\\alpha_d + c_d}{\\alpha_d + \\beta_d + i}$\n    \\State $\\hat{p}_{f\\mid d} \\leftarrow \\dfrac{\\alpha_{f\\mid d} + c_{f\\wedge d}}{\\alpha_{f\\mid d} + \\beta_{f\\mid d} + c_d}$\n    \\State $\\hat{p}_{f\\mid nd} \\leftarrow \\dfrac{\\alpha_{f\\mid nd} + c_{f\\wedge nd}}{\\alpha_{f\\mid nd} + \\beta_{f\\mid nd} + c_{nd}}$\n    \\State $\\hat{p}_f \\leftarrow \\hat{p}_{f\\mid d}\\hat{p}_d + \\hat{p}_{f\\mid nd}(1-\\hat{p}_d)$\n    \\State $\\hat{p}_{d\\mid f}(i) \\leftarrow \\dfrac{\\hat{p}_{f\\mid d}\\hat{p}_d}{\\hat{p}_f}$\n  \\EndFor\n  \\State \\textbf{return} $\\{\\hat{p}_{d\\mid f}(i)\\}_{i=1}^n$\n\\EndProcedure\n\\end{algorithmic}\n\\end{algorithm}\n```\n\nWe see that the posterior converged to a value a bit higher than our first analytical estimate. We accumulate much more evidence and, thus, were able to arrive at a more precise estimate to our posterior. In our toy example, an event being flagged doesn't mean, necessarily, that our workpiece will be defective; in fact, there's around $15\\%$ chance of it being the case. Note, however, that this is not the same as to say that both events are not correlated.\n\n",
    "supporting": [
      "handling-uncertainty_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}