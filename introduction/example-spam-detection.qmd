---
title: "Example: Spam Detection"
---

::: {#exm-spam-detection}
A spam detection system classifies incoming messages as spam or non-spam based on the presence of specific words. The system uses a fixed vocabulary and is trained on example messages labeled as spam or non-spam.

::: {layout-ncol=2}

```{python}
#| label: tbl-spam-vocab
#| tbl-cap: Vocabulary

from IPython.display import Markdown
from tabulate import tabulate
vocab = [[i, w] for i, w in enumerate([
	"word",
	"secret",
	"offer",
	"low",
	"price",
	"valued",
	"customer",
	"today",
	"dollar",
	"million",
	"sports",
	"is",
	"for",
	"play",
	"healthy",
	"pizza",
])]
Markdown(tabulate(vocab, headers=["id", "word"]))
```

```{python}
#| label: tbl-spam-data
#| tbl-cap: Spam dataset

from IPython.display import Markdown
from tabulate import tabulate
dataset = [
	["spam","million dollar offer"],
	["spam","secret offer today"],
	["spam","secret is secret"],
	["spam","low price for secret valued customer"],
	["non-spam","play sports today"],
	["non-spam","sports costs dollar"],
	["non-spam","healthy pizza for customer"],
	["non-spam","low price for valued customer"],
	["non-spam","play secret sports today"],
	["non-spam","sports is healthy"],
	["non-spam","low price pizza"],
]
Markdown(tabulate(dataset, headers=["label","message"]))
```

:::

This example is based on the exercise 3.22 from @murphyMachineLearningProbabilistic2014.
:::

**Goal.** Our goal is to build a spam detection system capable of classifying the messages as spam or not spam. Let us formulate the task:

$$
p(y_i = c \vert \mathbf{x}_i, \pmb{\Theta}) = ? 
$$

Where $p(y_i = c)$ is the probability of the message $i$ being of class $c$ in $\mathcal{C}$; $\pmb{\Theta}$ is the collection of all model parameters; $\mathbf{x}_i$ is the vector of features we want to classify, i.e., the output of some kind of builder $B$ that decomposes the message from @tbl-spam-data in terms of $\mathcal{V}$ from @tbl-spam-vocab. Expressing the task in terms of @eq-bayes, we get:

$$
p(y_i = c \vert \mathbf{x}_i, \pmb{\Theta}) \propto p(\mathbf{x}_i\vert y_i=c, \pmb{\Theta})p(y_i=c\vert \pmb{\Theta})
$$ {#eq-exm-spam-task}

**Building the feature vector $\mathbf{x}$.** Since we only have two classes, spam and non-spam, let us apply a builder $B_\mathbb{I}: \mathcal{D} \to \{0,1\}^{|\mathcal{V}|}$ that transforms the data set into binary feature vectors indicating the presence or absence of each word in the vocabulary. With this transformation, we can model each $x_i$ as a joint distribution of Bernoulli distributions of the corresponding feature for each word in $\mathcal{V}$, $$x_i\vert y_i=c \sim \bigcap_j^{|\mathcal{V}|} ber(\theta_{jc}).$$ 
**Assumption for the likelihood.** Assuming that all $x_{ij}$ are conditionally independent given the classes $C$, we can express the likelihood as:
$$
\forall x_a, x_b \in \mathbf{x}_i, x_a \perp x_b \vert y_i=c \implies\\
p(\mathbf{x}_i\vert y_i=c, \pmb{\Theta}) = \prod_j \theta_{jc}^{x_{ij}}(1-\theta_{jc})^{1-x_{ij}}
$$ {#eq-exm-spam-likelihood}

where $\theta_{jc} = p(x_{ij}=1\vert y_i=c), \theta_{jc} \in \pmb{\Theta}$ is the probability of word $j$ appearing in messages of class $c$.

**Assumption for the prior.** For now, let us use a uniform prior for the classes:
$$
p(y_i=c\vert \pmb{\Theta}) = \frac{1}{|\mathcal{C}|} = \hat{\pi}_c
$$ {#eq-exm-spam-prior}

where $|\mathcal{C}|$ is the number of classes. @eq-exm-spam-prior assumes that each class is equaly likely to show up in the population. However, this can be a too strong of an assumption for our example of sorting messages as spam and not-spam (think: are we equally likely to receive spam and non-spam messages?).

## Training the model: Computing $\pmb{\Theta}$

To use the equations above, we first need to compute our parameters $\pmb{\Theta}$. For that, we want to use our dataset from @tbl-spam-data represented as $\mathcal{D}$ to learn the parameters $\pmb{\Theta}$ of our model. Applying a similar bayesian treatment, we get:
$$
p(\pmb{\Theta}\vert \mathcal{D}) \propto p(\mathcal{D}\vert \pmb{\Theta})p(\pmb{\Theta})
$$ {#eq-exm-spam-training-likelihood}

**Computing the training likelihood $p(\mathcal{D}\vert\pmb{\Theta})$.** Since $\mathcal{D}\subseteq \mathbf{X}\times \mathcal{Y}$, using @eq-prob we can represent the likelihood of each data point (message) $(\mathbf{x}_i,y_i)$ as:
$$
p(\mathbf{x}_i,y_i\vert \pmb{\Theta}) = p(y_i\vert \pmb{\pi})p(\mathbf{x}_i\vert y_i, \pmb{\theta})  
$$ {#eq-exm-spam-training-likelihood-2}



::: {.callout-note appearance="simple"}

## Notation for parameters $\pmb{\Theta}$

Let $\pmb{\Theta} = \{\pmb{\pi}, \pmb{\theta}\}$ be the collection of all model parameters, where:

**Class Prior Parameters $\pmb{\pi}$.** The vector $\pmb{\pi}$ contains the prior probabilities for each class in $\mathcal{C}$. 
$$
\pmb{\pi} = \{\pi_c\}_{c \in \mathcal{C}} \quad \text{where} \quad \pi_c = p(y_i=c)
$$

**Feature-class Probabilities $\pmb{\theta}$.** The matrix $\pmb{\theta}$ contains the probabilities of each feature (word) given each class (spam/non-spam).
$$
\pmb{\theta} = \{\theta_{jc}\}_{j \in \mathcal{V}, c \in \mathcal{C}} \quad \text{where} \quad \theta_{jc} = p(x_{ij}=1\vert y_i=c)
$$

:::


Assuming that all features are conditionally independent given the class, we can expand the term $p(\mathbf{x}_i\vert y_i, \pmb{\theta})$ in @eq-exm-spam-training-likelihood-2 as follows:
$$
p(\mathbf{x}_i,y_i\vert \pmb{\theta}) = p(y_i\vert \pmb{\pi}) \prod_j p(x_{ij}\vert y_i, \theta_j) 
$$ {#eq-exm-spam-training-likelihood-3}

<!-- hm. it's still confusing how pi appeared here -->


To compute the matrix of probabilities $\pmb{\theta}$, we can use the maximum likelihood estimate (MLE) for each word $j$ in the vocabulary and each class $c$:

$$
\hat{\theta}_{jc} = \frac{\sum_{i} x_{ij} \mathbb{I}(y_i=c)}{\sum_{i} \mathbb{I}(y_i=c)}
$$ {#eq-exm-spam-mle}

Where $\mathbb{I}(\cdot)$ is the indicator function that returns 1 if the condition is true and 0 otherwise. 

::: {#chl-black-swan}

## Black Swan

However, using the MLE directly suffers from the **black swan problem**: This is analogous to the classical black swan problem: just because we've never observed a black swan doesn't mean the probability of seeing one should be exactly zero. It assigns extreme probabilities (exactly 0 or 1) when a word never appears or always appears in messages of a particular class. This leads to several issues:

1. **Zero probabilities** ($\theta_{jc} = 0$) cause numerical problems, e.g. when computing log-probabilities, since $\log(0) = -\infty$.
2. **Probabilities of 1** ($\theta_{jc} = 1$) make the model overly confident and brittle. If a word that always appeared in spam suddenly appears in a non-spam message, the likelihood becomes exactly 0, since we compute the product, completely ignoring all other evidence.
3. **Poor generalization**: The model becomes unable to handle unseen word-class combinations, even when they are plausible.

:::

**Laplace Smoothing.** To address @chl-black-swan, we can use **Laplace smoothing** (also called add-one smoothing):

$$
\hat{\theta}_{jc} = \frac{\sum_{i} x_{ij} \mathbb{I}(y_i=c) + \alpha}{\sum_{i} \mathbb{I}(y_i=c) + \alpha |\mathcal{V}|}
$$ {#eq-exm-spam-mle-smoothed}

where $\alpha > 0$ is the smoothing parameter (typically $\alpha=1$) and $|\mathcal{V}|$ is the size of the vocabulary. This ensures all probabilities are strictly in the interval $(0, 1)$, preventing both numerical instability and overconfident predictions.

Let us compute the matrix $\pmb{\hat{\theta}}$ using the dataset from @tbl-spam-data, considering that we have two classes: "spam" and "non-spam", where $c=1$ for "spam" and $c=0$ for "non-spam".

```{python}
#| label: tbl-spam-theta
#| tbl-cap: Matrix of probabilities $\pmb{\hat{\theta}}$
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
data = pd.DataFrame(dataset, columns=["label","message"])
vectorizer = CountVectorizer(vocabulary=[w for i,w in vocab])
X = vectorizer.fit_transform(data["message"]).toarray()
y = data["label"].values
classes = np.unique(y)
theta = np.zeros((len(vocab), len(classes)))
theta_no_laplace = np.zeros((len(vocab), len(classes)))
alpha = 1 # Laplace smoothing parameter
for j, w in enumerate(vocab):
	for c in classes:
		# Add Laplace smoothing to avoid zero probabilities
		theta[j, np.where(classes==c)[0][0]] = (np.sum(X[y==c, j]) + alpha) / (np.sum(y==c) + alpha * len(vocab))
		theta_no_laplace[j, np.where(classes==c)[0][0]] = (np.sum(X[y==c, j])) / (np.sum(y==c))	
# Both estimations in the same dataframe
theta_df = pd.DataFrame(theta, index=[w for i,w in vocab], columns=classes)
theta_no_laplace_df = pd.DataFrame(theta_no_laplace, index=[w for i,w in vocab], columns=classes)
theta_combined_df = pd.concat([theta_df, theta_no_laplace_df], axis=1, keys=["W/ Laplace Smoothing", "W/O Laplace Smoothing"])
theta_combined_df
```

The resulting matrix $\pmb{\theta}$ indicates the probability of each word in the vocabulary given the class (spam or non-spam). For example, the word "secret" has a high probability of appearing in spam messages, while "sports" is more likely to appear in non-spam messages.

## Making Predictions

To make predictions on new messages, we can use the learned parameters from @tbl-spam-theta and apply @eq-exm-spam-task. For the existing messages in @tbl-spam-data, we can compute the posterior probabilities for each class and classify the messages accordingly.

```{python}
#| label: tbl-spam-predictions
#| tbl-cap: Predictions on the spam dataset
predictions = []
for i, row in data.iterrows():
	x_i = vectorizer.transform([row["message"]]).toarray()[0]
	posteriors = {}
	for c in classes:
		likelihood = np.prod([theta[j, np.where(classes==c)[0][0]]**x_ij * (1-theta[j, np.where(classes==c)[0][0]])**(1-x_ij) for j, x_ij in enumerate(x_i)])
		prior = 1 / len(classes)
		posteriors[c] = likelihood * prior
	# Format posterior probabilities in scientific notation for clearer display
	posteriors = {c: f"{p:.2e}" for c, p in posteriors.items()}
	predicted_class = max(posteriors, key=posteriors.get)
	predictions.append((row["message"], row["label"], predicted_class, posteriors))
predictions_df = pd.DataFrame(predictions, columns=["message", "true_label", "predicted_label", "posteriors"])
predictions_df
```

From @tbl-spam-predictions, we can identify a few issues with our current model. The most glaring one is that our posteriors are extremely low, which could be a sign of numerical underflow when multiplying many small probabilities together. This is a common issue in Naive Bayes classifiers, increasing with the size of vocabulary, since all probabilities of the likelihood $\sum_x p(\mathbf{x}\vert y)$ should add to $1$. To mitigate this, we could consider using log for out computations, as shown in the next section.

:::{#chl-numerical-underflow}

## Numerical Underflow

Numerical underflow occurs when small numbers are rounded to zero in a computer. In the context of our spam detection model, when we compute the likelihood $p(\mathbf{x}_i\vert y_i=c, \pmb{\theta})$ as a product of many small probabilities (especially with a large vocabulary), the result can become so small that it is effectively treated as zero by the computer. This leads to incorrect posterior probabilities and unreliable classifications.
:::


## Avoiding Numerical Underflow

**Logarithms as a Solution for Numerical Underflow.** To avoid numerical underflow, we can compute the logarithm of the posterior probabilities instead of the probabilities themselves. This transforms the product of probabilities into a sum of logarithms, which is numerically more stable. The logarithm of the posterior probability can be expressed as:

$$
\begin{align*}
\log p(y_i = c \vert \mathbf{x}_i, \pmb{\theta}) &\propto \log p(\mathbf{x}_i\vert y_i=c, \pmb{\theta}) + \log p(y_i=c\vert \pmb{\theta}) \\
&\propto \sum_j \left[ x_{ij} \log \theta_{jc} + (1-x_{ij}) \log (1-\theta_{jc}) \right] + \log \hat{\pi}_c
\end{align*}
$$ {#eq-exm-spam-log-posterior}

That means, we can maximize the log-posterior instead of the posterior itself and use it to classify messages. Let us recompute the predictions using log-probabilities.

```{python}
#| label: tbl-spam-log-predictions
#| tbl-cap: Predictions using log-probabilities
predictions_log = []
for i, row in data.iterrows():
	x_i = vectorizer.transform([row["message"]]).toarray()[0]
	log_posteriors = {}
	for c in classes:
		log_likelihood = np.sum([x_ij * np.log(theta[j, np.where(classes==c)[0][0]]) + (1-x_ij) * np.log(1-theta[j, np.where(classes==c)[0][0]]) for j, x_ij in enumerate(x_i)])
		log_prior = np.log(1 / len(classes))
		log_posteriors[c] = log_likelihood + log_prior
	# Format log-posterior probabilities in scientific notation for clearer display
	log_posteriors = {c: f"{p:.2e}" for c, p in log_posteriors.items()}
	predicted_class = max(log_posteriors, key=log_posteriors.get)
	predictions_log.append((row["message"], row["label"], predicted_class, log_posteriors))
predictions_log_df = pd.DataFrame(predictions_log, columns=["message", "true_label", "predicted_label", "log_posteriors"])
predictions_log_df
```


**Not as expected?** We see that the number of false predictions for the second model is `{python} int((predictions_df["predicted_label"] != predictions_df["true_label"]).sum())`, while for the log-probability model is `{python} int((predictions_log_df["predicted_label"] != predictions_log_df["true_label"]).sum())`. Our log model performs worse! This is likely due to the extremely low posterior probabilities in both models, which makes it difficult to distinguish between classes. To improve performance, we may need to revisit our assumptions.

## Relaxing assumption for the prior

In @eq-exm-spam-prior, we assumed a uniform prior for the classes. However, this may not reflect the true distribution of spam and non-spam messages in practice. To relax this assumption, we can
