[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Safety Playbook",
    "section": "",
    "text": "Preface\nThis is a collection of notes about machine learning, the safety of machine learning and other related topics.\nPlease feel free to raise an issue if you find any error/typo/mistake or want to discuss a topic covered here.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "introduction/intro-uncertainty.html",
    "href": "introduction/intro-uncertainty.html",
    "title": "1  Introduction to Uncertainty",
    "section": "",
    "text": "1.1 Learning goals",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Uncertainty</span>"
    ]
  },
  {
    "objectID": "introduction/intro-uncertainty.html#learning-goals",
    "href": "introduction/intro-uncertainty.html#learning-goals",
    "title": "1  Introduction to Uncertainty",
    "section": "",
    "text": "Foundation of probabilistic reasoning for understanding uncertainty.\nUnderstand the Bayes Theorem and its components: prior, likelihood, posterior.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Uncertainty</span>"
    ]
  },
  {
    "objectID": "introduction/intro-uncertainty.html#bayesian-interpretation-of-uncertainty",
    "href": "introduction/intro-uncertainty.html#bayesian-interpretation-of-uncertainty",
    "title": "1  Introduction to Uncertainty",
    "section": "1.2 Bayesian Interpretation of Uncertainty",
    "text": "1.2 Bayesian Interpretation of Uncertainty\nInstead of viewing probabilities in terms of the frequencies of random, repeatable events, what is referred as the frequentist interpretation of probability, one can see them as proxy for uncertainty quantification. That means, that new events serve as new data points – evidence! – for improving the understanding of uncertainty in a process of interest.\n\nDefinition 1.1 (Random Variable) The events \\(x\\) and \\(y\\) can be understood as outcomes of the random variables \\(X\\) and \\(Y\\), respectively. Random variables are functions that maps our event space \\(\\Omega\\) to event outcomes, e.g., \\(x\\) and \\(y\\). When we write \\(p(X=x)\\) we are referring to the probability that the random variable function \\(X\\) is evaluated to \\(x\\). When the context is clear, we can abuse the notation and just write \\(p(x)\\) or \\(p(y)\\).\n\n\n1.2.1 Probability rules: Sum Rule and Product Rule\nThe sum rule is also known as Law of Total Probability. Bishop (2006) expresses them as:\n\nTheorem 1.1 (Sum rule/Law of Total Probability) \\[\np(X) = \\sum_Y p(X,Y)\n\\tag{1.1}\\]\n\n\nTheorem 1.2 (Product Rule/Definition of Conditional Probability) \\[\np(X,Y)=p(Y\\vert X)p(X)\n\\tag{1.2}\\]\nWhere \\(p(X,Y)\\) is the joint probability of random variables \\(X\\) and \\(Y\\), also commonly expressed as their intersection \\(p(X\\cap Y)\\).\n\nFrom Equation 1.2 we obtain the Bayes rule expressed in Equation 1.3.\n\nTheorem 1.3 (Bayes Rule) The probability of an event \\(y\\) happening, conditioned on knowing that event \\(x\\) happened can be expressed in the following form: \\[\n\\overbrace{p(y\\vert x)}^\\text{posterior}=\\frac{\\overbrace{p(x\\vert y)}^\\text{likelihood}\\overbrace{p(y)}^\\text{prior}}{\\underbrace{p(x)}_\\text{normalizing term}}.\n\\tag{1.3}\\]\n\n\nProof. Let \\(X\\) and \\(Y\\) be random variables. Applying Equation 1.2 for both \\(p(X,Y)\\) and \\(p(Y,X)\\), we have that: \\[\np(X,Y)=p(Y\\vert X)p(X) \\quad \\text{and} \\quad p(Y,X)=p(X\\vert Y)p(Y).\n\\] Equating both expressions and rearranging, we obtain that: \\[\n\\begin{aligned}\np(Y\\vert X)p(X) &= p(X\\vert Y)p(Y) \\\\\n\\implies p(Y\\vert X) &= \\frac{p(X\\vert Y)p(Y)}{p(X)},\n\\end{aligned}\n\\] which is the expression in Equation 1.3.\n\nAnother common representation of Bayes rule is obtained by applying the sum rule to the normalizing term \\(p(x)\\) in Equation 1.3, yielding: \\[\np(y\\vert x) = \\frac{p(x\\vert y)p(y)}{\\sum_{y_i} p(x\\vert y_i)p(y_i)}.\n\\tag{1.4}\\]\nEquation 1.4 is particularly useful because it expresses the denominator in the same terms as the numerator.\n\n\n1.2.2 Developing the Intuition For Understanding Uncertainty\nLooking at Equation 1.3, we see that the probability of \\(y\\) happening given that \\(x\\) happened depends on the probability of what we knew before about the process, \\(p(x)\\), which is commonly referred as the priors and the likelihood of it happening, \\(p(x\\vert y)\\). What we compute is the posterior, that is, the updated probability of \\(y\\) happening given \\(x\\) having happened.\nIn the next section, we will explore examples of classification and regression problems where we can apply the Bayesian interpretation for understanding uncertainty.\n\n\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Information Science and Statistics. New York: Springer.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Uncertainty</span>"
    ]
  },
  {
    "objectID": "introduction/probability-distributions.html",
    "href": "introduction/probability-distributions.html",
    "title": "2  Probability Distributions",
    "section": "",
    "text": "2.1 Bernoulli",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "introduction/probability-distributions.html#binomial",
    "href": "introduction/probability-distributions.html#binomial",
    "title": "2  Probability Distributions",
    "section": "2.2 Binomial",
    "text": "2.2 Binomial",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "introduction/probability-distributions.html#multinomial-generalization-of-binomial",
    "href": "introduction/probability-distributions.html#multinomial-generalization-of-binomial",
    "title": "2  Probability Distributions",
    "section": "2.3 Multinomial (generalization of Binomial)",
    "text": "2.3 Multinomial (generalization of Binomial)\n\nDefinition 2.1 (Multinomial Distribution) The multinomial distribution is a generalization of the binomial distribution. It models the probability of counts for each of \\(k\\) different outcomes in \\(n\\) independent trials, where each trial results in exactly one of the \\(k\\) outcomes. Let \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_k) \\in \\mathbf{X}\\) be the vector of counts for each outcome and \\(\\mathbf{X}\\) the vector of random variables, we say that \\[\\mathbf{X}\\sim \\text{Mu}_k(n,\\pmb{\\theta}).\\] The probability mass function (PMF) of the multinomial distribution is given by: \\[\n\\begin{aligned}\np(\\mathbf{X}=\\mathbf{x}) &= \\binom{n}{x_1, x_2, \\ldots, x_k} \\theta_1^{x_1} \\theta_2^{x_2} \\ldots \\theta_k^{x_k}\\\\\n&= \\binom{n}{x_1, x_2, \\ldots, x_k} \\prod_{i=1}^{k} \\theta_i^{x_i}\n\\end{aligned}\n\\tag{2.1}\\] where:\n\n\\(n\\) is the total number of trials,\n\\(x_i\\) is the count of occurrences of outcome \\(i\\) (with \\(\\sum_{i=1}^{k} x_i = n\\)),\n\\(p_i\\) is the probability of outcome \\(i\\) (with \\(\\sum_{i=1}^{k} \\theta_i = 1\\)).\nThe term \\[\\binom{n}{x_1, x_2, \\ldots, x_k} \\triangleq \\frac{n!}{x_1! x_2! \\ldots x_k!}\\] is the multinomial coefficient, which counts the number of ways to arrange the counts of the different outcomes.\n\n\n\nExample 2.1 (5 ones and 5 sixes in 10 rolls of a fair die) Suppose we roll a fair six-sided die 10 times. We want to find the probability of getting 5 ones and 5 sixes: 1 appears 5 times, 2 appears 0 times, 3 appears 0 times, 4 appears 0 times, 5 appears 0 times, and 6 appears 5 times. Let \\(\\mathbf{x} = (5, 0, 0, 0, 0, 5)\\) be the vector of counts for each face. Since the die is fair, the probabilities for each face are equal: \\[\\pmb{\\theta} = \\left(\\frac{1}{6}, \\frac{1}{6}, \\frac{1}{6}, \\frac{1}{6}, \\frac{1}{6}, \\frac{1}{6}\\right).\\] We can say that \\[\\mathbf{X}\\sim \\text{Mu}_6(10,\\pmb{\\theta}).\\] By applying Equation 2.1, we can calculate the probability of observing the counts in \\(\\mathbf{x}\\): \\[\n\\begin{aligned}\np(\\mathbf{X}=\\mathbf{x}) &= \\binom{10}{5, 0, 0, 0, 0, 5} \\left(\\frac{1}{6}\\right)^5 \\left(\\frac{1}{6}\\right)^0 \\left(\\frac{1}{6}\\right)^0 \\left(\\frac{1}{6}\\right)^0 \\left(\\frac{1}{6}\\right)^0 \\left(\\frac{1}{6}\\right)^5\\\\\n&= \\frac{10!}{5! 0! 0! 0! 0! 5!} \\left(\\frac{1}{6}\\right)^{10} \\approx 4.17\\times 10^{-6}\\\\\n\\end{aligned}\n\\]\nYou can expect to be this unlucky about once every 240,000 times you repeat this experiment!\n\nA special case of the multinomial distribution is the categorical distribution, which models the outcome of a single trial with \\(k\\) possible outcomes.\n\nDefinition 2.2 (Categorical Distribution) The categorical distribution is a special case of the multinomial distribution where there is only one trial (\\(n=1\\)). It models the probability of each of \\(k\\) different outcomes in a single trial. Let \\(X\\) be a random variable representing the outcome of the trial, we say that \\[X\\sim \\text{Cat}_k(\\pmb{\\theta}) \\triangleq \\text{Mu}_k(1,\\pmb{\\theta}).\\]\nNote that since we have just one experiment, the vector of outcomes must contain exactly one non-zero element equal to one. For example, \\(\\mathbf{x}=(1,0,\\ldots,0)\\) indicates that outcome 1 occurred, \\(\\mathbf{x}=(0,1,0,\\ldots,0)\\) indicates that outcome 2 occurred, and so on.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "introduction/probability-distributions.html#gaussian-normal-distribution",
    "href": "introduction/probability-distributions.html#gaussian-normal-distribution",
    "title": "2  Probability Distributions",
    "section": "2.4 Gaussian: (Normal) Distribution",
    "text": "2.4 Gaussian: (Normal) Distribution",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "introduction/probability-distributions.html#conjugate-priors",
    "href": "introduction/probability-distributions.html#conjugate-priors",
    "title": "2  Probability Distributions",
    "section": "2.5 Conjugate Priors",
    "text": "2.5 Conjugate Priors\n\n2.5.1 Beta-Bernoulli\n\n\n2.5.2 Dirichlet-Multinomial\n\n\n2.5.3 And Gaussian?",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "introduction/example-anomaly-detection.html",
    "href": "introduction/example-anomaly-detection.html",
    "title": "3  Example: Anomaly Detection",
    "section": "",
    "text": "3.1 Extending the example with online update\nTo make the discussion concrete, consider the following toy scenario. Suppose that, historically, \\(2\\%\\) of the parts produced by the machine turn out defective (a really bad rate, but useful for the demonstration). This means: \\[\np(d)=0.02\n\\]\nWe also happen to know from historical data that when a defect is present, the current sensor raises a flag \\(90\\%\\) of the time, but it also generates false alarms: even when the part is fine, it still flags \\(15\\%\\) of the time. Formally: \\[\np(f\\vert d)  = 0.9 \\text{, }\np(f\\vert nd) = 0.15\n\\]\nFrom Equation 1.2 and Equation 1.1, we can compute \\(p(f)\\):\n\\[\np(f)=p(f\\vert d)p(d) + p(f\\vert nd)p(nd)\n\\]\nBayes’ rule lets us update the belief about a part being defective as soon as the sensor fires. Applying Equation 1.3 and since \\(Y\\) can only assume one of the values in the set \\(\\{d,nd\\}\\), i.e., \\(p(nd)=1-p(d)\\), we have the following equation in function of the variables we already know:\n\\[\np(d\\vert f)=\\frac{p(f\\vert d)p(d)}{p(f)}=\\frac{p(f\\vert d)p(d)}{p(f\\vert d)p(d) + p(f\\vert nd)(1-p(d))}\n\\]\nThis computes to \\(p(d\\vert f) =\\) 0.11, which is still surprisingly low given our prior and likelihood. The method, however, forces us to be realistic in our belief and say that we either need more evidence to prove \\(A\\) as a metric for \\(Y\\) or we should pivot to other experiments. Since our result depended so strongly on our choice of priors, we can assume that reliably collecting this information is critically important for significant outcomes using the method. Indeed, this is a common critique about the Bayesian approach to probability: it’s often more difficult to be reproduced in comparison to frequentist approaches, since the research outcome depends heavily on these initial assumptions.\nThe previous example computed an analytical estimate for our posterior, making no explicit assumption about the underlying probability distribution of the prior and likelihood, treating them as fixed point estimates based on historical data.\nThe bayesian approach conceptualizes probabilities as uncertainty. This is useful for online learning scenarios, where part of the dataset is encountered during runtime. In order to apply online learning to Example 3.1, let us first formally define our new learning task:\n\\[\np(\\theta_{f,d} \\vert \\ \\mathcal{D}_f)=?\n\\]\nWhere \\(\\theta_{f,d}\\) is the probability of the part being defective given the sensor reading, \\(\\mathcal{D}_f=\\{D_1, D_2, \\dots, D_i\\}\\) is the set of observations of \\(D\\) outcomes when \\(F=f\\). Interestingly, \\(\\theta_{f,d}\\) is both a probability and a random variable itself, since we are uncertain about its value.\nWe can once again express our task in terms of the bayesian equation (Equation 1.3), but now making use of proportionality since, for the moment, we are not interested in the normalizing constant \\(p(\\mathcal{D}_f)\\): \\[\n\\begin{align*}\np(\\theta_{f,d} \\vert \\ \\mathcal{D}_f)&\\propto p(\\mathcal{D}_f\\vert \\theta_{f,d})p(\\theta_{f,d}) \\\\\n&\\propto p(\\mathcal{D}_f\\vert \\theta_{f,d})(\\theta_{f,d})\n\\end{align*}\n\\tag{3.1}\\]",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Example: Anomaly Detection</span>"
    ]
  },
  {
    "objectID": "introduction/example-anomaly-detection.html#extending-the-example-with-online-update",
    "href": "introduction/example-anomaly-detection.html#extending-the-example-with-online-update",
    "title": "3  Example: Anomaly Detection",
    "section": "",
    "text": "3.1.1 Modelling the prior and likelihood as beta-bernoulli distributions\nNow, we need ways to express both our likelihood and prior as probability distributions, instead of point estimates, since we want to update our belief as new data points are observed. For that, we use the concept of conjugate priors, which are pairs of probability distributions that, when used together in Bayes’ rule, yield a posterior distribution of the same family as the prior distribution. This property greatly simplifies the process of updating beliefs with new evidence.\nIn our case, we can use the Beta distribution as our prior and the Bernoulli distribution as our likelihood. Specifically, we can model the prior distribution of \\(\\theta_{f,d}\\) as a Beta distribution, \\(Beta(a, b)\\), and the likelihood of the observed data as a Bernoulli distribution, \\(Ber(\\theta_{f,d})\\).\n\\[\n\\mathcal{D}_f\\vert \\theta_{f,d} \\sim Ber(\\theta_{f,d})\n\\] \\[\n\\theta_{f,d} \\sim Beta(a, b)\n\\]\nWhere \\(\\sim\\) means that the random variable follows the probability distribution on the right side. The Beta distribution is defined on the interval \\([0, 1]\\) and is parameterized by two positive shape parameters, \\(a\\) and \\(b\\), commonly referred as hyperparameters and they are used to encode our beliefs into the prior distribution. It is often used to model the distribution of probabilities. The Bernoulli distribution, on the other hand, models binary outcomes (success/failure) and is parameterized by a single probability parameter \\(\\theta\\).\nWe can write our likelihood and prior as: \\[p(\\mathcal{D}_f\\vert \\theta_{f,d})=\\theta_{f,d}^{N_{f,d}}(1-\\theta_{f,d})^{N_{f,nd}}\\] \\[p(\\theta_{f,d})\\propto \\theta_{f,d}^{a-1}(1-\\theta_{f,d})^{b-1}\\]\nHere, \\(N_{f,d}\\) and \\(N_{f,nd}\\) are the counts of flagged defective and non-defective events. Plugging in these expressions into Equation 3.1, we get: \\[\n\\begin{align*}\np(\\theta_{f,d} \\vert \\ \\mathcal{D}_f)&\\propto Ber(\\theta_{f,d})Beta(a, b) \\\\\n&\\propto \\theta_{f,d}^{N_{f,d}}(1-\\theta_{f,d})^{N_{f,nd}}\\theta_{f,d}^{a-1}(1-\\theta_{f,d})^{b-1} \\\\\n&\\propto \\theta_{f,d}^{N_{f,d}+a-1}(1-\\theta_{f,d})^{N_{f,nd}+b-1} \\\\\n&\\propto Beta(a+N_{f,d}, b+N_{f,nd})\n\\end{align*}\n\\tag{3.2}\\]\nWe see that the posterior also follows a Beta distribution, with updated parameters \\(a+N_{f,d}\\) and \\(b+N_{f,nd}\\). This means that we can update our belief about the probability of a part being defective by simply updating the parameters of the Beta distribution as we observe new data. Therefore, Bayesian inference can be used both sequentially or in batch mode with equivalent results, which it makes it suitable for online learning scenarios. For a proof of this property, see Murphy (2014). From Murphy’s book, we can also look at useful properties of the Beta distribution, such as its mean, mode and variance: \\[\n\\mathbb{E}[\\theta]=\\frac{a}{a+b}, \\quad mode(\\theta)=\\frac{a-1}{a+b-2}, \\quad Var(\\theta)=\\frac{ab}{(a+b)^2(a+b+1)}\n\\tag{3.3}\\]\nBased on that, we can compute the mode, or MAP estimate, of our posterior distribution as: \\[\n\\begin{align*}\n\\hat{\\theta}_{f,d}\\vert \\ \\mathcal{D}_f&=\\frac{a+N_{f,d}-1}{a+b+N_{f,d}+N_{f,nd}-2} \\\\\n&=\\frac{a+N_{f,d}-1}{a+b+N_f-2} \\\\\n\\end{align*}\n\\tag{3.4}\\]\nWhere \\(N_f=N_{f,d}+N_{f,nd}\\) is the total number of flagged events. Let us plot the prior, likelihood and posterior distributions for our initial example. We can choose the hyperparameters \\(a\\) and \\(b\\) such that the prior mean matches our initial belief of \\(p(d)=0.02\\). A good choice is to set \\(a=2\\) and \\(b=98\\), which gives us the same prior mean of \\(2/(2+98)=0.02\\). For the likelihood, we can use the counts of flagged defective and non-defective events based on our initial example. Assuming we observed \\(90\\) flagged defective events and \\(15\\) flagged non-defective events, we have \\(N_{f,d}=90\\) and \\(N_{f,nd}=15\\). Thus, our posterior will have parameters \\(a+N_{f,d}=2+90=92\\) and \\(b+N_{f,nd}=98+15=113\\).",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Example: Anomaly Detection</span>"
    ]
  },
  {
    "objectID": "introduction/example-anomaly-detection.html#online-updates-of-beta-bernoulli-model",
    "href": "introduction/example-anomaly-detection.html#online-updates-of-beta-bernoulli-model",
    "title": "3  Example: Anomaly Detection",
    "section": "3.2 Online updates of beta-bernoulli model",
    "text": "3.2 Online updates of beta-bernoulli model\nTo see how new observations gradually nudge our posterior beliefs, the next simulation follows 1000 consecutive parts using the Algorithm 1. After each part, the posterior is updated and the mode and the error bar is computed based on Equation 3.3. The error bar is computed as three times the standard deviation (\\(3\\sigma\\)) around the mode, assuming a latent true latent defect probability of flagged events of \\(\\theta_{f,d}^\\star=0.6\\).\n\n\n\n\n\n\n\n\n\nOur error bar shrinks as we observe more data, reflecting our increased confidence in the estimate. The mode converges towards the true latent defect probability of flagged events, \\(\\theta_{f,d}^\\star=0.6\\), demonstrating how Bayesian updating refines our beliefs with accumulating evidence.\n\n\n\\begin{algorithm} \\caption{Sequential Beta-Bernoulli Update for $\\theta_{f,d}$}\\begin{algorithmic} \\INPUT $a_0 \\geq 1, b_0 \\geq 1$, prior Beta hyperparameters \\INPUT $\\{d_i\\}_{i=1}^T$, sequence of flagged defect observations \\OUTPUT $\\{\\hat{\\theta}_{f,d}(t)\\}_{t=1}^T$, $\\{\\sigma(t)\\}_{t=1}^T$ \\Procedure{BetaBernoulliUpdate}{$a_0, b_0, \\{d_i\\}_{i=1}^T$} \\State $a \\leftarrow a_0$ \\Comment{initialize posterior parameters} \\State $b \\leftarrow b_0$ \\For{$t = 1$ \\To $T$} \\State $d \\leftarrow d_i$ \\Comment{observe whether part is defective} \\If{$d = \\texttt{defect}$} \\State $a \\leftarrow a + 1$ \\Comment{increment success count} \\Else \\State $b \\leftarrow b + 1$ \\Comment{increment failure count} \\EndIf \\State $\\hat{\\theta}_{f,d}(t) \\leftarrow \\dfrac{a - 1}{a + b - 2}$ \\Comment{MAP estimate (mode)} \\State $\\sigma^2(t) \\leftarrow \\dfrac{ab}{(a+b)^2(a+b+1)}$ \\Comment{posterior variance} \\State $\\sigma(t) \\leftarrow \\sqrt{\\sigma^2(t)}$ \\EndFor \\State \\textbf{return} $\\{\\hat{\\theta}_{f,d}(t)\\}_{t=1}^T, \\{\\sigma(t)\\}_{t=1}^T$ \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\n\n\nNoteGeneralization\n\n\n\nThis algorithm assumes \\(a_0 \\geq 1\\) and \\(b_0 \\geq 1\\), which corresponds to using the Beta distribution to encode pseudo-counts or prior observations. For more general priors (e.g., Jeffreys prior with \\(a_0 = b_0 = 0.5\\)), the MAP estimate should be replaced with the posterior mean \\(\\frac{a}{a+b}\\) when \\(a \\leq 1\\) or \\(b \\leq 1\\), since the mode is undefined in those cases.\n\n\n\n\n\n\nMurphy, Kevin P. 2014. Machine Learning - A Probabilistic Perspective. Adaptive Computation and Machine Learning. Cambridge: MIT Press.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Example: Anomaly Detection</span>"
    ]
  },
  {
    "objectID": "introduction/example-spam-detection.html",
    "href": "introduction/example-spam-detection.html",
    "title": "4  Example: Spam Detection",
    "section": "",
    "text": "4.1 Training the model: Computing \\(\\pmb{\\Theta}\\) via MLE\nGoal. Our goal is to build a spam detection system capable of classifying the messages as spam or not spam. Let us formulate the task:\n\\[\np(y_i = c \\vert \\mathbf{x}_i, \\pmb{\\Theta}) = ?\n\\]\nWhere \\(p(y_i = c)\\) is the probability of the message \\(i\\) being of class \\(c\\) in \\(\\mathcal{C}\\); \\(\\pmb{\\Theta}\\) is the collection of all model parameters; \\(\\mathbf{x}_i\\) is the vector of features we want to classify, i.e., the output of some kind of builder \\(B\\) that decomposes the message from Table 4.2 in terms of \\(\\mathcal{V}\\) from Table 4.1. Expressing the task in terms of Equation 1.3, we get:\n\\[\np(y_i = c \\vert \\mathbf{x}_i, \\pmb{\\Theta}) \\propto p(\\mathbf{x}_i\\vert y_i=c, \\pmb{\\Theta})p(y_i=c\\vert \\pmb{\\Theta})\n\\tag{4.1}\\]\nBuilding the feature vector \\(\\mathbf{x}\\). Since we only have two classes, spam and non-spam, let us apply a builder \\(B_\\mathbb{I}: \\mathcal{D} \\to \\{0,1\\}^{|\\mathcal{V}|}\\) that transforms the data set into binary feature vectors indicating the presence or absence of each word in the vocabulary. With this transformation, we can model each \\(x_i\\) as a joint distribution of Bernoulli distributions of the corresponding feature for each word in \\(\\mathcal{V}\\), \\[x_{ij}\\vert y_i=c \\sim ber(\\theta_{jc}).\\] Assumption for the likelihood. Assuming that all \\(x_{ij}\\) are conditionally independent given the classes \\(C\\), we can express the likelihood as: \\[\n\\forall x_a, x_b \\in \\mathbf{x}_i, x_a \\perp x_b \\vert y_i=c \\implies\\\\\np(\\mathbf{x}_i\\vert y_i=c, \\pmb{\\Theta}) = \\prod_j \\theta_{jc}^{x_{ij}}(1-\\theta_{jc})^{1-x_{ij}}\n\\tag{4.2}\\]\nwhere \\(\\theta_{jc} = p(x_{ij}=1\\vert y_i=c), \\theta_{jc} \\in \\pmb{\\Theta}\\) is the probability of word \\(j\\) appearing in messages of class \\(c\\).\nAssumption for the prior. For now, let us use a uniform prior for the classes: \\[\np(y_i=c\\vert \\pmb{\\Theta}) = \\frac{1}{|\\mathcal{C}|} = \\hat{\\pi}_c\n\\tag{4.3}\\]\nwhere \\(|\\mathcal{C}|\\) is the number of classes. Equation 4.3 assumes that each class is equaly likely to show up in the population. However, this can be a too strong of an assumption for our example of sorting messages as spam and not-spam (think: are we equally likely to receive spam and non-spam messages?).\nTo use the equations above, we first need to compute our parameters \\(\\pmb{\\Theta}\\). For that, we want to use our dataset from Table 4.2 represented as \\(\\mathcal{D}\\) to learn the parameters \\(\\pmb{\\Theta}\\) of our model. Applying a similar bayesian treatment, we get: \\[\np(\\pmb{\\Theta}\\vert \\mathcal{D}) \\propto p(\\mathcal{D}\\vert \\pmb{\\Theta})p(\\pmb{\\Theta})\n\\tag{4.4}\\]\nWe can use the maximum likelihood estimate (MLE) to compute the parameters in \\(\\pmb{\\Theta}\\). Later on, we will discuss the limitations of this approach and how we can improve it.\nComputing the training likelihood \\(p(\\mathcal{D}\\vert\\pmb{\\Theta})\\). Since \\(\\mathcal{D}\\subseteq \\mathbf{X}\\times \\mathcal{Y}\\), using Equation 1.2 we can represent the likelihood of each data point (message) \\((\\mathbf{x}_i,y_i)\\) as: \\[\np(\\mathbf{x}_i,y_i\\vert \\pmb{\\Theta}) = p(y_i\\vert \\pmb{\\pi})p(\\mathbf{x}_i\\vert y_i, \\pmb{\\theta})  \n\\tag{4.5}\\]\nBy using the assumption in Equation 4.2, we can expand the term \\(p(\\mathbf{x}_i\\vert y_i, \\pmb{\\theta})\\) in Equation 4.5 as follows: \\[\np(\\mathbf{x}_i,y_i\\vert \\pmb{\\Theta}) = \\prod_c p(y_i=c\\vert \\pi_c) \\prod_j\\prod_c p(x_{ij}\\vert y_i=c, \\theta_{jc})\n\\tag{4.6}\\]\nBy using Proposition A.1, we can rewrite the equation above as:\n\\[\np(\\mathbf{x}_i,y_i\\vert \\pmb{\\Theta}) = \\prod_c \\pi_c^{\\mathbb{I}(y_i=c)} \\prod_j\\prod_c \\theta_{jc}^{\\mathbb{I}(y_i=c)}(1-\\theta_{jc})^{1-\\mathbb{I}(y_i=c)}\n\\tag{4.7}\\]\nReturning to Equation 4.4, assuming independence between data points, we can express the full likelihood of the dataset \\(\\mathcal{D}\\) as:\n\\[\np(\\mathcal{D}\\vert \\pmb{\\Theta}) = \\prod_{i} p(\\mathbf{x}_i,y_i\\vert \\pmb{\\Theta})\n\\tag{4.8}\\]\nPlugging Equation 4.7 into Equation 4.8, and taking the logarithm to simplify the product into a sum, we get: \\[\n\\log p(\\mathcal{D}\\vert \\pmb{\\Theta}) = \\sum_{i} \\sum_c \\mathbb{I}(y_i=c) \\log \\pi_c + \\sum_{i} \\sum_j \\sum_c [ \\mathbb{I}(y_i=c) \\log \\theta_{jc} +(1-\\mathbb{I}(y_i=c)) \\log (1-\\theta_{jc})]\n\\tag{4.9}\\]\nComputing the MLE for \\(\\theta_{jc}\\). To compute the MLE for \\(\\theta_{jc}\\), we need to maximize the log-likelihood from Equation 4.9 with respect to the parameters \\(\\theta_{jc}\\).\nTo do this, we can take the derivative of the log-likelihood with respect to \\(\\theta_{jc}\\), set it to zero, and solve for \\(\\theta_{jc}\\). This gives us the following equation:\n\\[\n\\frac{\\partial}{\\partial \\theta_{jc}} \\log p(\\mathcal{D}\\vert \\pmb{\\Theta}) = \\sum_{i} \\sum_c \\mathbb{I}(y_i=c) \\frac{1}{\\theta_{jc}} - \\sum_{i} \\sum_c (1-\\mathbb{I}(y_i=c)) \\frac{1}{1-\\theta_{jc}} = 0\n\\]\nWhich leads to the MLE for \\(\\theta_{jc}\\) as follows:\n\\[\n\\begin{aligned}\n\\hat{\\theta}_{jc} &= \\frac{\\sum_{i} x_{ij} \\mathbb{I}(y_i=c)}{\\sum_{i} \\mathbb{I}(y_i=c)}\n\\end{aligned}\n\\]\nTo compute the matrix of probabilities \\(\\pmb{\\theta}\\), we can use the maximum likelihood estimate (MLE) for each word \\(j\\) in the vocabulary and each class \\(c\\):\n\\[\n\\hat{\\theta}_{jc} = \\frac{\\sum_{i} x_{ij} \\mathbb{I}(y_i=c)}{\\sum_{i} \\mathbb{I}(y_i=c)}\n\\tag{4.10}\\]\nWhere \\(\\mathbb{I}(\\cdot)\\) is the indicator function that returns 1 if the condition is true and 0 otherwise.\nLaplace Smoothing. To address Challenge 4.1, we can use Laplace smoothing (also called add-one smoothing):\n\\[\n\\hat{\\theta}_{jc} = \\frac{\\sum_{i} x_{ij} \\mathbb{I}(y_i=c) + \\alpha}{\\sum_{i} \\mathbb{I}(y_i=c) + \\alpha |\\mathcal{V}|}\n\\tag{4.11}\\]\nwhere \\(\\alpha &gt; 0\\) is the smoothing parameter (typically \\(\\alpha=1\\)) and \\(|\\mathcal{V}|\\) is the size of the vocabulary. This ensures all probabilities are strictly in the interval \\((0, 1)\\), preventing both numerical instability and overconfident predictions.\nLet us compute the matrix \\(\\pmb{\\hat{\\theta}}\\) using the dataset from Table 4.2, considering that we have two classes: “spam” and “non-spam”, where \\(c=1\\) for “spam” and \\(c=0\\) for “non-spam”.\nTable 4.3: Matrix of probabilities \\(\\pmb{\\hat{\\theta}}\\)\n\n\n\n\n\n\n\n\n\n\nW/ Laplace Smoothing\nW/O Laplace Smoothing\n\n\n\nnon-spam\nspam\nnon-spam\nspam\n\n\n\n\nword\n0.043478\n0.05\n0.000000\n0.00\n\n\nsecret\n0.086957\n0.25\n0.142857\n1.00\n\n\noffer\n0.043478\n0.15\n0.000000\n0.50\n\n\nlow\n0.130435\n0.10\n0.285714\n0.25\n\n\nprice\n0.130435\n0.10\n0.285714\n0.25\n\n\nvalued\n0.086957\n0.10\n0.142857\n0.25\n\n\ncustomer\n0.130435\n0.10\n0.285714\n0.25\n\n\ntoday\n0.130435\n0.10\n0.285714\n0.25\n\n\ndollar\n0.086957\n0.10\n0.142857\n0.25\n\n\nmillion\n0.043478\n0.10\n0.000000\n0.25\n\n\nsports\n0.217391\n0.05\n0.571429\n0.00\n\n\nis\n0.086957\n0.10\n0.142857\n0.25\n\n\nfor\n0.130435\n0.10\n0.285714\n0.25\n\n\nplay\n0.130435\n0.05\n0.285714\n0.00\n\n\nhealthy\n0.130435\n0.05\n0.285714\n0.00\n\n\npizza\n0.130435\n0.05\n0.285714\n0.00\nThe resulting matrix \\(\\pmb{\\theta}\\) indicates the probability of each word in the vocabulary given the class (spam or non-spam). For example, the word “secret” has a high probability of appearing in spam messages, while “sports” is more likely to appear in non-spam messages.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Example: Spam Detection</span>"
    ]
  },
  {
    "objectID": "introduction/example-spam-detection.html#training-the-model-computing-pmbtheta-via-mle",
    "href": "introduction/example-spam-detection.html#training-the-model-computing-pmbtheta-via-mle",
    "title": "4  Example: Spam Detection",
    "section": "",
    "text": "NoteNotation for parameters \\(\\pmb{\\Theta}\\)\n\n\n\nLet \\(\\pmb{\\Theta} = \\{\\pmb{\\pi}, \\pmb{\\theta}\\}\\) be the collection of all model parameters, where:\nClass Prior Parameters \\(\\pmb{\\pi}\\). The vector \\(\\pmb{\\pi}\\) contains the prior probabilities for each class in \\(\\mathcal{C}\\). \\[\n\\pmb{\\pi} = \\{\\pi_c\\}_{c \\in \\mathcal{C}} \\quad \\text{where} \\quad \\pi_c = p(y_i=c)\n\\]\nFeature-class Probabilities \\(\\pmb{\\theta}\\). The matrix \\(\\pmb{\\theta}\\) contains the probabilities of each feature (word) given each class (spam/non-spam). \\[\n\\pmb{\\theta} = \\{\\theta_{jc}\\}_{j \\in \\mathcal{V}, c \\in \\mathcal{C}} \\quad \\text{where} \\quad \\theta_{jc} = p(x_{ij}=1\\vert y_i=c)\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 4.1 (Black Swan) However, using the MLE directly suffers from the black swan problem: This is analogous to the classical black swan problem: just because we’ve never observed a black swan doesn’t mean the probability of seeing one should be exactly zero. It assigns extreme probabilities (exactly 0 or 1) when a word never appears or always appears in messages of a particular class. This leads to several issues:\n\nZero probabilities (\\(\\theta_{jc} = 0\\)) cause numerical problems, e.g. when computing log-probabilities, since \\(\\log(0) = -\\infty\\).\nProbabilities of 1 (\\(\\theta_{jc} = 1\\)) make the model overly confident and brittle. If a word that always appeared in spam suddenly appears in a non-spam message, the likelihood becomes exactly 0, since we compute the product, completely ignoring all other evidence.\nPoor generalization: The model becomes unable to handle unseen word-class combinations, even when they are plausible.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Example: Spam Detection</span>"
    ]
  },
  {
    "objectID": "introduction/example-spam-detection.html#making-predictions",
    "href": "introduction/example-spam-detection.html#making-predictions",
    "title": "4  Example: Spam Detection",
    "section": "4.2 Making Predictions",
    "text": "4.2 Making Predictions\nTo make predictions on new messages, we can use the learned parameters from Table 4.3 and apply Equation 4.1. For the existing messages in Table 4.2, we can compute the posterior probabilities for each class and classify the messages accordingly.\n\n\n\n\nTable 4.4: Predictions on the spam dataset\n\n\n\n\n\n\n\n\n\n\nmessage\ntrue_label\npredicted_label\nposteriors\n\n\n\n\n0\nmillion dollar offer\nspam\nspam\n{'non-spam': '1.53e-05', 'spam': '2.08e-04'}\n\n\n1\nsecret offer today\nspam\nspam\n{'non-spam': '5.05e-05', 'spam': '6.25e-04'}\n\n\n2\nsecret is secret\nspam\nnon-spam\n{'non-spam': '6.72e-05', 'spam': '1.18e-03'}\n\n\n3\nlow price for secret valued customer\nspam\nspam\n{'non-spam': '3.57e-07', 'spam': '5.39e-07'}\n\n\n4\nplay sports today\nnon-spam\nnon-spam\n{'non-spam': '4.86e-04', 'spam': '2.94e-05'}\n\n\n5\nsports costs dollar\nnon-spam\nspam\n{'non-spam': '2.06e-03', 'spam': '5.59e-04'}\n\n\n6\nhealthy pizza for customer\nnon-spam\nnon-spam\n{'non-spam': '3.94e-05', 'spam': '3.27e-06'}\n\n\n7\nlow price for valued customer\nnon-spam\nnon-spam\n{'non-spam': '3.75e-06', 'spam': '1.62e-06'}\n\n\n8\nplay secret sports today\nnon-spam\nspam\n{'non-spam': '4.63e-05', 'spam': '9.80e-06'}\n\n\n9\nsports is healthy\nnon-spam\nnon-spam\n{'non-spam': '3.09e-04', 'spam': '2.94e-05'}\n\n\n10\nlow price pizza\nnon-spam\nspam\n{'non-spam': '2.63e-04', 'spam': '6.21e-05'}\n\n\n\n\n\n\n\n\n\n\nFrom Table 4.4, we can identify a few issues with our current model. The most glaring one is that our posteriors are extremely low, which could be a sign of numerical underflow when multiplying many small probabilities together. This is a common issue in Naive Bayes classifiers, increasing with the size of vocabulary, since all probabilities of the likelihood \\(\\sum_x p(\\mathbf{x}\\vert y)\\) should add to \\(1\\). To mitigate this, we could consider using log for out computations, as shown in the next section.\n\nChallenge 4.2 (Numerical Underflow) Numerical underflow occurs when small numbers are rounded to zero in a computer. In the context of our spam detection model, when we compute the likelihood \\(p(\\mathbf{x}_i\\vert y_i=c, \\pmb{\\theta})\\) as a product of many small probabilities (especially with a large vocabulary), the result can become so small that it is effectively treated as zero by the computer. This leads to incorrect posterior probabilities and unreliable classifications.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Example: Spam Detection</span>"
    ]
  },
  {
    "objectID": "introduction/example-spam-detection.html#avoiding-numerical-underflow",
    "href": "introduction/example-spam-detection.html#avoiding-numerical-underflow",
    "title": "4  Example: Spam Detection",
    "section": "4.3 Avoiding Numerical Underflow",
    "text": "4.3 Avoiding Numerical Underflow\nLogarithms as a Solution for Numerical Underflow. To avoid numerical underflow, we can compute the logarithm of the posterior probabilities instead of the probabilities themselves. This transforms the product of probabilities into a sum of logarithms, which is numerically more stable. The logarithm of the posterior probability can be expressed as:\n\\[\n\\begin{align*}\n\\log p(y_i = c \\vert \\mathbf{x}_i, \\pmb{\\theta}) &\\propto \\log p(\\mathbf{x}_i\\vert y_i=c, \\pmb{\\theta}) + \\log p(y_i=c\\vert \\pmb{\\theta}) \\\\\n&\\propto \\sum_j \\left[ x_{ij} \\log \\theta_{jc} + (1-x_{ij}) \\log (1-\\theta_{jc}) \\right] + \\log \\hat{\\pi}_c\n\\end{align*}\n\\tag{4.12}\\]\nThat means, we can maximize the log-posterior instead of the posterior itself and use it to classify messages. Let us recompute the predictions using log-probabilities.\n\n\n\n\nTable 4.5: Predictions using log-probabilities\n\n\n\n\n\n\n\n\n\n\nmessage\ntrue_label\npredicted_label\nlog_posteriors\n\n\n\n\n0\nmillion dollar offer\nspam\nspam\n{'non-spam': '-1.11e+01', 'spam': '-8.48e+00'}\n\n\n1\nsecret offer today\nspam\nnon-spam\n{'non-spam': '-9.89e+00', 'spam': '-7.38e+00'}\n\n\n2\nsecret is secret\nspam\nnon-spam\n{'non-spam': '-9.61e+00', 'spam': '-6.74e+00'}\n\n\n3\nlow price for secret valued customer\nspam\nnon-spam\n{'non-spam': '-1.48e+01', 'spam': '-1.44e+01'}\n\n\n4\nplay sports today\nnon-spam\nnon-spam\n{'non-spam': '-7.63e+00', 'spam': '-1.04e+01'}\n\n\n5\nsports costs dollar\nnon-spam\nspam\n{'non-spam': '-6.19e+00', 'spam': '-7.49e+00'}\n\n\n6\nhealthy pizza for customer\nnon-spam\nspam\n{'non-spam': '-1.01e+01', 'spam': '-1.26e+01'}\n\n\n7\nlow price for valued customer\nnon-spam\nspam\n{'non-spam': '-1.25e+01', 'spam': '-1.33e+01'}\n\n\n8\nplay secret sports today\nnon-spam\nnon-spam\n{'non-spam': '-9.98e+00', 'spam': '-1.15e+01'}\n\n\n9\nsports is healthy\nnon-spam\nnon-spam\n{'non-spam': '-8.08e+00', 'spam': '-1.04e+01'}\n\n\n10\nlow price pizza\nnon-spam\nspam\n{'non-spam': '-8.24e+00', 'spam': '-9.69e+00'}\n\n\n\n\n\n\n\n\n\n\nNot as expected? We see that the number of false predictions for the second model is 4, while for the log-probability model is 7. Our log model performs worse! This is likely due to the extremely low posterior probabilities in both models, which makes it difficult to distinguish between classes. To improve performance, we may need to revisit our assumptions.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Example: Spam Detection</span>"
    ]
  },
  {
    "objectID": "introduction/example-spam-detection.html#relaxing-assumption-for-the-prior",
    "href": "introduction/example-spam-detection.html#relaxing-assumption-for-the-prior",
    "title": "4  Example: Spam Detection",
    "section": "4.4 Relaxing assumption for the prior",
    "text": "4.4 Relaxing assumption for the prior\nIn Equation 4.3, we assumed a uniform prior for the classes. However, this may not reflect the true distribution of spam and non-spam messages in practice. To relax this assumption, we can\n\n\n\n\nMurphy, Kevin P. 2014. Machine Learning - A Probabilistic Perspective. Adaptive Computation and Machine Learning. Cambridge: MIT Press.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Example: Spam Detection</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bishop, Christopher M. 2006. Pattern Recognition and Machine\nLearning. Information Science and Statistics. New York: Springer.\n\n\nMurphy, Kevin P. 2014. Machine Learning - A\nProbabilistic Perspective. Adaptive Computation\nand Machine Learning. Cambridge: MIT Press.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "appendix/propositions-theorems.html",
    "href": "appendix/propositions-theorems.html",
    "title": "Appendix A — Propositions and Theorems",
    "section": "",
    "text": "Proposition A.1 (One-Hot Indicator Equivalence for Categorical Conditionals) Let \\(Y\\) be a categorical random variable \\(Y \\sim \\text{cat}_k(\\pmb{\\pi})\\), taking values in \\(\\{1,\\dots,C\\}\\) with class probabilities \\(\\pi_c&gt;0\\), \\(\\sum_{c=1}^C\\pi_c=1, \\forall \\pi_c \\in \\pmb{\\pi}\\). Let a random variable \\(X=(X_1,\\dots,X_J)\\) and assume conditional independence of components given \\(Y\\) so that for each \\(c\\), \\[\np(X\\mid Y=c)=\\prod_{j=1}^J p(X_j\\mid Y=c).\n\\]\nDefine the one-hot indicator vector \\[\n\\mathbf{Y}=(Y_1,\\dots,Y_C),\\qquad Y_c=\\mathbb{1}\\{Y=c\\},\\qquad \\sum_{c=1}^C Y_c=1.\n\\] Then the two expressions \\[\n    \\text{(I)}\\qquad p(X,Y)=p(Y)\\prod_{j=1}^J p(X_j\\mid Y),\n\\qquad\n    \\text{(II)}\\qquad p(X,\\mathbf{Y})=\\prod_{c=1}^C\\Bigl[\\pi_c\\prod_{j=1}^J p(X_j\\mid \\mathbf{Y}=c)\\Bigr]^{Y_c}\n\\] are equivalent (they define the same joint probability function).\n\n\nProof. Let the realized class be \\(c^\\ast\\), so \\(Y=c^\\ast\\) and \\(Y_{c^\\ast}=1\\) while \\(Y_c=0\\) for \\(c\\neq c^\\ast\\). Evaluate (II) at that realization: \\[\n\\prod_{c=1}^C\\Bigl[\\pi_c\\prod_{j=1}^J p(X_j\\mid Y=c)\\Bigr]^{Y_c}\n= \\Bigl[\\pi_{c^\\ast}\\prod_{j=1}^J p(X_j\\mid Y=c^\\ast)\\Bigr]^{1}\n\\prod_{c\\neq c^\\ast}\\bigl[\\cdots\\bigr]^{0}.\n\\] Factors raised to the zero power equal \\(1\\), so the product reduces to \\[\n\\pi_{c^\\ast}\\prod_{j=1}^J p(X_j\\mid Y=c^\\ast),\n\\] which equals (I) evaluated at \\(Y=c^\\ast\\). Since the above holds for every possible realized class \\(c^\\ast\\), (I) and (II) coincide as functions of \\((X,Y)\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Propositions and Theorems</span>"
    ]
  }
]