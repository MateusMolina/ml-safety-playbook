[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Safety Playbook",
    "section": "",
    "text": "Preface\nThis is a collection of notes about machine learning, the safety of machine learning and other related topics.\nPlease feel free to raise an issue if you find any error/typo/mistake or want to discuss a topic covered here.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "1  Summary",
    "section": "",
    "text": "todo",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "introduction/handling-uncertainty.html",
    "href": "introduction/handling-uncertainty.html",
    "title": "2  Handling Uncertainty",
    "section": "",
    "text": "2.1 Learning goals",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Handling Uncertainty</span>"
    ]
  },
  {
    "objectID": "introduction/handling-uncertainty.html#learning-goals",
    "href": "introduction/handling-uncertainty.html#learning-goals",
    "title": "2  Handling Uncertainty",
    "section": "",
    "text": "Understand the Bayes Theorem as a foundational block for making sense of uncertainty.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Handling Uncertainty</span>"
    ]
  },
  {
    "objectID": "introduction/handling-uncertainty.html#bayesian-interpretation-of-uncertainty",
    "href": "introduction/handling-uncertainty.html#bayesian-interpretation-of-uncertainty",
    "title": "2  Handling Uncertainty",
    "section": "2.2 Bayesian Interpretation of Uncertainty",
    "text": "2.2 Bayesian Interpretation of Uncertainty\nInstead of viewing probabilities in terms of the frequencies of random, repeatable events, what is referred as the frequentist interpretation of probability, one can see them as proxy for uncertainty quantification. That means, that new events serve as new data points – evidence! – for improving the understanding of uncertainty in a process of interest.\n\n2.2.1 Bayes Equation\nLet us first formally define the Bayes equation.\n\nDefinition 2.1 The probability of an event \\(x\\) given knowledge of an event \\(y\\) can be expressed in the following form: \\[\np(y\\vert x)=\\frac{p(x\\vert y)p(y)}{p(x)}\n\\tag{2.1}\\]\n\nThe events \\(x\\) and \\(y\\) can be understood as outcomes of the random variables \\(X\\) and \\(Y\\), respectively. Random variables are functions that maps our event space \\(\\Omega\\) to event outcomes, e.g., \\(x\\) and \\(y\\). When we write \\(p(X=x)\\) we are referring to the probability that the random variable function \\(X\\) is evaluated to \\(x\\). When the context is clear, we can abuse the notation and just write \\(p(x)\\) or \\(p(y)\\).\nDefinition 2.1 is a consequence of two important probability rules: sum rule and product rule.\n\n\n2.2.2 Probability rules: Sum Rule and Product Rule\nThe sum rule is also known as Law of Total Probability. Bishop (2006) expresses them as:\n\\[\np(X) = \\sum_Y p(X,Y)\n\\tag{2.2}\\] \\[\np(X,Y)=p(Y\\vert X)p(X)\n\\tag{2.3}\\]\nWhere \\(p(X,Y)\\) is the joint probability of random variables \\(X\\) and \\(Y\\), also commonly expressed as their intersection \\(p(X\\cap Y)\\).\nThe product rule (Equation 2.3) is also the definition of conditional probability.\n\n\n2.2.3 Developing the Intuition For Understanding Uncertainty\nLooking at Equation 2.1, we see that the probability of \\(y\\) happening given that \\(x\\) happened depends on the probability of what we knew before about the process, \\(p(x)\\), which is commonly referred as the priors and the likelihood of it happening, \\(p(x\\vert y)\\). What we compute is the posterior, that is, the updated probability of \\(y\\) happening given \\(x\\) having happened.\n\nExample 2.1 (Anomaly Detection) A machine is equipped with an electric current sensor and a control system that flags if the electric current is outside of the typical range. We can assign a random variable \\(F\\) that maps boolean outcomes to events in \\(\\Omega\\), i.e., \\(F: \\Omega\\rightarrow\\{f,nf\\}\\), where \\(f\\) represents at least one flagged event and \\(nf\\) a non-flagged one, and \\(\\Omega\\) represents the event space for workpieces being processed in the machine.\nLet us assume that we monitor another property of this machine, namely, how often we have to discard a workpiece being produced there. We attribute another random variable \\(D\\) to this process that also maps \\(D:\\Omega\\rightarrow\\{d,nd\\}\\)\nWe want to continuously verify how effective our anomaly detection metric is in predicting defective workpieces, that is, monitor our \\(p(D=d\\vert F=f)\\) or simply \\(p(d|f)\\) for convenience. This will be our posterior. Ideally, if we want to implement a metric as proxy for defective workpieces, this posterior should be close to \\(1\\), i.e., all flagged events led to a defective workpiece.\n\nTo make the discussion concrete, consider the following toy scenario. Suppose that, historically, \\(2\\%\\) of the parts produced by the machine turn out defective (a really bad rate, but useful for the demonstration). This means: \\[\np(d)=0.02\n\\]\nWe also happen to know from historical data that when a defect is present, the current sensor raises a flag \\(90\\%\\) of the time, but it also generates false alarms: even when the part is fine, it still flags \\(15\\%\\) of the time. Formally: \\[\np(f\\vert d)  = 0.9 \\text{, }\np(f\\vert nd) = 0.15\n\\]\nFrom Equation 2.3 and Equation 2.2, we can compute \\(p(f)\\):\n\\[\np(f)=p(f\\vert d)p(d) + p(f\\vert nd)p(nd)\n\\]\nBayes’ rule lets us update the belief about a part being defective as soon as the sensor fires. Applying Equation 2.1 and since \\(Y\\) can only assume one of the values in the set \\(\\{d,nd\\}\\), i.e., \\(p(nd)=1-p(d)\\), we have the following equation in function of the variables we already know:\n\\[\np(d\\vert f)=\\frac{p(f\\vert d)p(d)}{p(f)}=\\frac{p(f\\vert d)p(d)}{p(f\\vert d)p(d) + p(f\\vert nd)(1-p(d))}\n\\]\nThis computes to \\(p(d\\vert f) =\\) 0.11, which is still surprisingly low given our prior and likelihood. The method, however, forces us to be realistic in our belief and say that we either need more evidence to prove \\(A\\) as a metric for \\(Y\\) or we should pivot to other experiments. Since our result depended so strongly on our choice of priors, we can assume that reliably collecting this information is critically important for significant outcomes using the method. Indeed, this is a common critique about the Bayesian approach to probability: it’s often more difficult to be reproduced in comparison to frequentist approaches, since the research outcome depends heavily on these initial assumptions.\n\n\n2.2.4 Extending the example with online update\nThe previous example computed an analytical estimate for our posterior, making no explicit assumption about the underlying probability distribution of the prior and likelihood, treating them as fixed point estimates based on historical data.\nThe bayesian approach conceptualizes probabilities as uncertainty. This is useful for online learning scenarios, where part of the dataset is encountered during runtime. In order to apply online learning to Example 2.1, let us first formally define our new learning task:\n\\[\np(\\theta_{f,d} \\vert \\ \\mathcal{D}_f)=?\n\\]\nWhere \\(\\theta_{f,d}\\) is the probability of the part being defective given the sensor reading, \\(\\mathcal{D}_f=\\{D_1, D_2, \\dots, D_i\\}\\) is the set of observations of \\(D\\) outcomes when \\(F=f\\). Interestingly, \\(\\theta_{f,d}\\) is both a probability and a random variable itself, since we are uncertain about its value.\nWe can once again express our task in terms of the bayesian equation (Equation 2.1): \\[\n\\begin{align*}\np(\\theta_{f,d} \\vert \\ \\mathcal{D}_f)&\\propto p(\\mathcal{D}_f\\vert \\theta_{f,d})p(\\theta_{f,d}) \\\\\n&\\propto p(\\mathcal{D}_f\\vert \\theta_{f,d})(\\theta_{f,d})\n\\end{align*}\n\\tag{2.4}\\]\n\n2.2.4.1 Modelling the prior and likelihood\nNow, we need ways to express both our likelihood and prior as probability distributions, instead of point estimates, since we want to update our belief as new data points are observed. For that, we use the concept of conjugate priors, which are pairs of probability distributions that, when used together in Bayes’ rule, yield a posterior distribution of the same family as the prior distribution. This property greatly simplifies the process of updating beliefs with new evidence.\nIn our case, we can use the Beta distribution as our prior and the Bernoulli distribution as our likelihood. Specifically, we can model the prior distribution of \\(\\theta_{f,d}\\) as a Beta distribution, \\(Beta(a, b)\\), and the likelihood of the observed data as a Bernoulli distribution, \\(Ber(\\theta_{f,d})\\).\n\\[\n\\mathcal{D}_f\\vert \\theta_{f,d} \\sim Ber(\\theta_{f,d})\n\\] \\[\n\\theta_{f,d} \\sim Beta(a, b)\n\\]\nWhere \\(\\sim\\) means that the random variable follows the probability distribution on the right side. The Beta distribution is defined on the interval \\([0, 1]\\) and is parameterized by two positive shape parameters, \\(a\\) and \\(b\\), commonly referred as hyperparameters and they are used to encode our beliefs into the prior distribution. It is often used to model the distribution of probabilities. The Bernoulli distribution, on the other hand, models binary outcomes (success/failure) and is parameterized by a single probability parameter \\(\\theta\\).\nWe can write our likelihood and prior as: \\[p(\\mathcal{D}_f\\vert \\theta_{f,d})=\\theta_{f,d}^{N_{f,d}}(1-\\theta_{f,d})^{N_{f,nd}}\\] \\[p(\\theta_{f,d})\\propto \\theta_{f,d}^{a-1}(1-\\theta_{f,d})^{b-1}\\]\nHere, \\(N_{f,d}\\) and \\(N_{f,nd}\\) are the counts of flagged defective and non-defective events. Plugging in these expressions into Equation 2.4, we get: \\[\n\\begin{align*}\np(\\theta_{f,d} \\vert \\ \\mathcal{D}_f)&\\propto Ber(\\theta_{f,d})Beta(a, b) \\\\\n&\\propto \\theta_{f,d}^{N_{f,d}}(1-\\theta_{f,d})^{N_{f,nd}}\\theta_{f,d}^{a-1}(1-\\theta_{f,d})^{b-1} \\\\\n&\\propto \\theta_{f,d}^{N_{f,d}+a-1}(1-\\theta_{f,d})^{N_{f,nd}+b-1} \\\\\n&\\propto Beta(a+N_{f,d}, b+N_{f,nd})\n\\end{align*}\n\\tag{2.5}\\]\nWe see that the posterior also follows a Beta distribution, with updated parameters \\(a+N_{f,d}\\) and \\(b+N_{f,nd}\\). This means that we can update our belief about the probability of a part being defective by simply updating the parameters of the Beta distribution as we observe new data. Therefore, Bayesian inference can be used both sequentially or in batch mode with equivalent results, which it makes it suitable for online learning scenarios. For a proof of this property, see Murphy (2014). From Murphy’s book, we can also look at useful properties of the Beta distribution, such as its mean, mode and variance: \\[\n\\mathbb{E}[\\theta]=\\frac{a}{a+b}, \\quad mode(\\theta)=\\frac{a-1}{a+b-2}, \\quad Var(\\theta)=\\frac{ab}{(a+b)^2(a+b+1)}\n\\tag{2.6}\\]\nBased on that, we can compute the mode, or MAP estimate, of our posterior distribution as: \\[\n\\begin{align*}\n\\hat{\\theta}_{f,d}\\vert \\ \\mathcal{D}_f&=\\frac{a+N_{f,d}-1}{a+b+N_{f,d}+N_{f,nd}-2} \\\\\n&=\\frac{a+N_{f,d}-1}{a+b+N_f-2} \\\\\n\\end{align*}\n\\tag{2.7}\\]\nWhere \\(N_f=N_{f,d}+N_{f,nd}\\) is the total number of flagged events. Let us plot the prior, likelihood and posterior distributions for our initial example. We can choose the hyperparameters \\(a\\) and \\(b\\) such that the prior mean matches our initial belief of \\(p(d)=0.02\\). A good choice is to set \\(a=2\\) and \\(b=98\\), which gives us the same prior mean of \\(2/(2+98)=0.02\\). For the likelihood, we can use the counts of flagged defective and non-defective events based on our initial example. Assuming we observed \\(90\\) flagged defective events and \\(15\\) flagged non-defective events, we have \\(N_{f,d}=90\\) and \\(N_{f,nd}=15\\). Thus, our posterior will have parameters \\(a+N_{f,d}=2+90=92\\) and \\(b+N_{f,nd}=98+15=113\\).\n\n\n\n\n\n\n\n\n\nTo see how new observations gradually nudge our posterior beliefs, the next simulation follows 1000 consecutive parts using the Algorithm 1. After each part, the posterior is updated and the mode and the error bar is computed based on Equation 2.6. The error bar is computed as three times the standard deviation (\\(3\\sigma\\)) around the mode, assuming a latent true latent defect probability of flagged events of \\(\\theta_{f,d}^\\star=0.6\\).\n\n\n\n\n\n\n\n\n\nOur error bar shrinks as we observe more data, reflecting our increased confidence in the estimate. The mode converges towards the true latent defect probability of flagged events, \\(\\theta_{f,d}^\\star=0.6\\), demonstrating how Bayesian updating refines our beliefs with accumulating evidence.\n\n\n\\begin{algorithm} \\caption{Sequential Beta-Bernoulli Update for $\\theta_{f,d}$}\\begin{algorithmic} \\INPUT $a_0 \\geq 1, b_0 \\geq 1$, prior Beta hyperparameters \\INPUT $\\{d_i\\}_{i=1}^T$, sequence of flagged defect observations \\OUTPUT $\\{\\hat{\\theta}_{f,d}(t)\\}_{t=1}^T$, $\\{\\sigma(t)\\}_{t=1}^T$ \\Procedure{BetaBernoulliUpdate}{$a_0, b_0, \\{d_i\\}_{i=1}^T$} \\State $a \\leftarrow a_0$ \\Comment{initialize posterior parameters} \\State $b \\leftarrow b_0$ \\For{$t = 1$ \\To $T$} \\State $d \\leftarrow d_i$ \\Comment{observe whether part is defective} \\If{$d = \\texttt{defect}$} \\State $a \\leftarrow a + 1$ \\Comment{increment success count} \\Else \\State $b \\leftarrow b + 1$ \\Comment{increment failure count} \\EndIf \\State $\\hat{\\theta}_{f,d}(t) \\leftarrow \\dfrac{a - 1}{a + b - 2}$ \\Comment{MAP estimate (mode)} \\State $\\sigma^2(t) \\leftarrow \\dfrac{ab}{(a+b)^2(a+b+1)}$ \\Comment{posterior variance} \\State $\\sigma(t) \\leftarrow \\sqrt{\\sigma^2(t)}$ \\EndFor \\State \\textbf{return} $\\{\\hat{\\theta}_{f,d}(t)\\}_{t=1}^T, \\{\\sigma(t)\\}_{t=1}^T$ \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\n\n\nNoteGeneralization\n\n\n\nThis algorithm assumes \\(a_0 \\geq 1\\) and \\(b_0 \\geq 1\\), which corresponds to using the Beta distribution to encode pseudo-counts or prior observations. For more general priors (e.g., Jeffreys prior with \\(a_0 = b_0 = 0.5\\)), the MAP estimate should be replaced with the posterior mean \\(\\frac{a}{a+b}\\) when \\(a \\leq 1\\) or \\(b \\leq 1\\), since the mode is undefined in those cases.\n\n\n\n\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Information Science and Statistics. New York: Springer.\n\n\nMurphy, Kevin P. 2014. Machine Learning - A Probabilistic Perspective. Adaptive Computation and Machine Learning. Cambridge: MIT Press.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Handling Uncertainty</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bishop, Christopher M. 2006. Pattern Recognition and Machine\nLearning. Information Science and Statistics. New York: Springer.\n\n\nMurphy, Kevin P. 2014. Machine Learning - A\nProbabilistic Perspective. Adaptive Computation\nand Machine Learning. Cambridge: MIT Press.",
    "crumbs": [
      "References"
    ]
  }
]