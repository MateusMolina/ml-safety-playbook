---
title: "Handling Uncertainty"
---

## Learning goals

1. Understand the Bayes Theorem as a foundational block for making sense of uncertainty.

## Bayesian Interpretation of Uncertainty

Instead of viewing probabilities in terms of the frequencies of random, repeatable events, what is referred as the _frequentist_ interpretation of probability, one can see them as proxy for uncertainty quantification. That means, that new events serve as new data points -- evidence! -- for improving the understanding of uncertainty in a process of interest.

### Bayes Theorem

Let us first formally define the Bayes Theorem

::: {#def-bayes}

The probability of an event $x$ given knowledge of an event $y$ can be expressed in the following form:
$$
p(y\vert x)=\frac{p(x\vert y)p(y)}{p(x)}
$$ {#eq-bayes}

:::

The events $x$ and $y$ can be understood as outcomes of the _random variables_ $X$ and $Y$, respectively. Random variables are functions that maps our event space $\Omega$ to event outcomes, e.g., $x$ and $y$. When we write $p(X=x)$ we are referring to the probability that the random variable function $X$ is evaluated to $x$. When the context is clear, we can abuse the notation and just write $p(x)$ or $p(y)$.

@def-bayes is a consequence of two important probability rules: _sum rule_ and _product rule_. 

### Probability rules: Sum Rule and Product Rule

The _sum rule_ is also known as _Law of Total Probability_. @bishopPatternRecognitionMachine2006 expresses them as:

$$
p(X) = \sum_Y p(X,Y)
$$ {#eq-sum}
$$
p(X,Y)=p(Y\vert X)p(X)
$$ {#eq-prob}

Where $p(X,Y)$ is the _joint probability_ of random variables $X$ and $Y$, also commonly expressed as their intersection $p(X\cap Y)$.

The _product rule_ (@eq-prob) is also the definition of conditional probability.

### Developing the Intuition For Understanding Uncertainty

Looking at @eq-bayes, we see that the probability of $y$ happening given that $x$ happened depends on the probability of what we knew before about the process, $p(x)$, which is commonly referred as the _priors_ and the _likelihood_ of it happening, $p(x\vert y)$. What we compute is the _posterior_, that is, the updated probability of $y$ happening given $x$ having happened.

::: {#exm-anomaly-detection}
## Anomaly Detection
A machine is equipped with an electric current sensor and a control system that flags if the electric current is outside of the typical range. We can assign a random variable $A$ (anomaly) that maps boolean outcomes to events in $\Omega$, i.e., $A: \Omega\rightarrow\{f,nf\}$, where $f$ represents at least one flagged event and $nf$ a non-flagged one, and $\Omega$ represents the event space for workpieces being processed in the machine.

Let us assume that we monitor another property of this machine, namely, how often we have to discard a workpiece being produced there. We attribute another random variable $Y$ (outcome) to this process that also maps $Y:\Omega\rightarrow\{d,nd\}$ 

We want to continuously verify how effective our anomaly detection metric is in predicting defective workpieces, that is, monitor our $p(Y=d\vert A=f)$ or simply $p(d|f)$ for convenience. This will be our _posterior_. Ideally, if we want to implement a metric as proxy for defective workpieces, this posterior should be close to $1$, i.e., all flagged events led to a defective workpiece. 
::: 

To make the discussion concrete, consider the following toy scenario. Suppose that, historically, $2\%$ of the parts produced by the machine turn out defective (a really bad rate, but useful for the demonstration). This means:
$$
p(d)=0.02
$$

We also happen to know from historical data that when a defect is present, the current sensor raises a flag $90\%$ of the time, but it also generates false alarms: even when the part is fine, it still flags $15\%$ of the time. Formally: 
$$
p(f\vert d)  = 0.9 \text{, }
p(f\vert nd) = 0.15 
$$

From @eq-prob and @eq-sum, we can compute $p(f)$:

$$
p(f)=p(f\vert d)p(d) + p(f\vert nd)p(nd)
$$

Bayes' rule lets us update the belief about a part being defective as soon as the sensor fires. Applying @eq-bayes and since $Y$ can only assume one of the values in the set $\{d,nd\}$, i.e., $p(nd)=1-p(d)$, we have the following equation in function of the variables we already know:

$$
p(d\vert f)=\frac{p(f\vert d)p(d)}{p(f)}=\frac{p(f\vert d)p(d)}{p(f\vert d)p(d) + p(f\vert nd)(1-p(d))}
$$

This computes to $p(d\vert f) =$ `{python} f"{0.9*0.02/(0.9*0.02+0.15*(1-0.02)):.2f}"`, which is still surprisingly low given our prior and likelihood. The method, however, forces us to be realistic in our belief and say that we either need more _evidence_ to prove $A$ as a metric for $Y$ or we should pivot to other experiments. Since our result depended so strongly on our choice of priors, we can assume that reliably collecting this information is critically important for significant outcomes using the method. Indeed, this is a common critique about the Bayesian approach to probability: it's often more difficult to be reproduced in comparison to frequentist approaches, since the research outcome depends heavily on these initial assumptions.

To see how new observations gradually nudge our belief, the next simulation follows 1000 consecutive parts using the @algo-bayesian-update. After each part, we recompute the quantities in Bayes' rule using the empirical frequencies observed so far.

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

np.random.seed(60)

p_defect = 0.02
p_flag_given_defect = 0.90
p_flag_given_ok = 0.15

p_flag = p_flag_given_defect * p_defect + p_flag_given_ok * (1 - p_defect)
posterior_true = p_flag_given_defect * p_defect / p_flag

prior_defect_alpha, prior_defect_beta = 2, 98
prior_flag_defect_alpha, prior_flag_defect_beta = 9, 1
prior_flag_ok_alpha, prior_flag_ok_beta = 3, 17

n_events = 1000
defects_seen = 0
flagged_defects = 0
flagged_ok = 0

posterior_estimates = []

for event in range(n_events):
    is_defect = np.random.rand() < p_defect
    if is_defect:
        flag = np.random.rand() < p_flag_given_defect
    else:
        flag = np.random.rand() < p_flag_given_ok

    defects_seen += int(is_defect)
    ok_seen = (event + 1) - defects_seen

    if flag:
        if is_defect:
            flagged_defects += 1
        else:
            flagged_ok += 1

    defect_alpha = prior_defect_alpha + defects_seen
    defect_beta = prior_defect_beta + ok_seen
    p_defect_hat = defect_alpha / (defect_alpha + defect_beta)

    flag_defect_alpha = prior_flag_defect_alpha + flagged_defects
    flag_defect_beta = prior_flag_defect_beta + max(defects_seen - flagged_defects, 0)
    p_flag_given_defect_hat = flag_defect_alpha / (flag_defect_alpha + flag_defect_beta)

    flag_ok_alpha = prior_flag_ok_alpha + flagged_ok
    flag_ok_beta = prior_flag_ok_beta + max(ok_seen - flagged_ok, 0)
    p_flag_given_ok_hat = flag_ok_alpha / (flag_ok_alpha + flag_ok_beta)

    p_flag_hat = (
        p_flag_given_defect_hat * p_defect_hat
        + p_flag_given_ok_hat * (1 - p_defect_hat)
    )

    posterior_estimate = (
        p_flag_given_defect_hat * p_defect_hat / p_flag_hat
        if p_flag_hat > 0
        else np.nan
    )
    posterior_estimates.append(posterior_estimate)

posterior_line = pd.Series(posterior_estimates).to_numpy()

fig, ax = plt.subplots(figsize=(8, 4))
ax.plot(range(1, n_events + 1), posterior_line, label="Empirical posterior", color="#4C78A8")
ax.axhline(posterior_true, linestyle="--", color="#F58518", label="Analytical posterior")
ax.set_ylim(0, max(0.4, np.nanmax(posterior_line) * 1.2))
plt.show(fig)
```

```pseudocode
#| label: algo-bayesian-update

\begin{algorithm}
\caption{Sequential Bayesian Update for $p(d\mid f)$}
\begin{algorithmic}[1]
\Require $\alpha_d, \beta_d$ \Comment{prior counts for $p(d)$}
\Require $\alpha_{f\mid d}, \beta_{f\mid d}$ \Comment{prior counts for $p(f\mid d)$}
\Require $\alpha_{f\mid nd}, \beta_{f\mid nd}$ \Comment{prior counts for $p(f\mid nd)$}
\Require $\{(f_i, d_i)\}_{i=1}^n$ \Comment{sequence of observations}
\Ensure $\{\hat{p}_{d\mid f}(i)\}_{i=1}^n$ \Comment{posterior after each observation}
\Procedure{UpdatePosterior}{$\alpha_d, \beta_d, \alpha_{f\mid d}, \beta_{f\mid d}, \alpha_{f\mid nd}, \beta_{f\mid nd}, \{(f_i, d_i)\}_{i=1}^n$}
  \State $c_d \leftarrow 0$ \Comment{defects observed}
  \State $c_{f\wedge d} \leftarrow 0$ \Comment{flagged defects}
  \State $c_{f\wedge nd} \leftarrow 0$ \Comment{false alarms}
  \For{$i = 1$ \To $n$}
    \State $(f, d) \leftarrow (f_i, d_i)$ \Comment{$f_i \in \{\texttt{flag}, \texttt{no\_flag}\}$}
    \State $c_d \leftarrow c_d + \mathbb{I}[d = \texttt{defect}]$
    \State $c_{nd} \leftarrow i - c_d$
    \If{$f = \texttt{flag}$}
      \If{$d = \texttt{defect}$}
        \State $c_{f\wedge d} \leftarrow c_{f\wedge d} + 1$
      \Else
        \State $c_{f\wedge nd} \leftarrow c_{f\wedge nd} + 1$
      \EndIf
    \EndIf
    \State $\hat{p}_d \leftarrow \dfrac{\alpha_d + c_d}{\alpha_d + \beta_d + i}$
    \State $\hat{p}_{f\mid d} \leftarrow \dfrac{\alpha_{f\mid d} + c_{f\wedge d}}{\alpha_{f\mid d} + \beta_{f\mid d} + c_d}$
    \State $\hat{p}_{f\mid nd} \leftarrow \dfrac{\alpha_{f\mid nd} + c_{f\wedge nd}}{\alpha_{f\mid nd} + \beta_{f\mid nd} + c_{nd}}$
    \State $\hat{p}_f \leftarrow \hat{p}_{f\mid d}\hat{p}_d + \hat{p}_{f\mid nd}(1-\hat{p}_d)$
    \State $\hat{p}_{d\mid f}(i) \leftarrow \dfrac{\hat{p}_{f\mid d}\hat{p}_d}{\hat{p}_f}$
  \EndFor
  \State \textbf{return} $\{\hat{p}_{d\mid f}(i)\}_{i=1}^n$
\EndProcedure
\end{algorithmic}
\end{algorithm}
```

We see that the posterior converged to a value a bit higher than our first analytical estimate. We accumulate much more evidence and, thus, were able to arrive at a more precise estimate to our posterior. In our toy example, an event being flagged doesn't mean, necessarily, that our workpiece will be defective; in fact, there's around $15\%$ chance of it being the case. Note, however, that this is not the same as to say that both events are not correlated.